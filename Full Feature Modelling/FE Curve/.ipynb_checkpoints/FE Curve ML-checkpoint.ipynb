{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost \n",
    "import lightgbm\n",
    "import catboost\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import ensemble\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "###########wrapping root mean square error for later calls##########\n",
    "def compute_mae_mse_rmse(target,prediction):\n",
    "    error = []\n",
    "    for i in range(len(target)):\n",
    "        error.append(target[i] - prediction[i])\n",
    "    squaredError = []\n",
    "    absError = []\n",
    "    for val in error:\n",
    "        squaredError.append(val * val)  # target-prediction之差平方\n",
    "        absError.append(abs(val))  # 误差绝对值\n",
    "    mae=sum(absError)/len(absError)  # 平均绝对误差MAE\n",
    "    mse=sum(squaredError)/len(squaredError)  # 均方误差MSE\n",
    "    RMSE=np.sqrt(sum(squaredError)/len(squaredError))\n",
    "    R2=r2_score(target,prediction)\n",
    "    return mae,mse,RMSE,R2\n",
    "###########loading data##########\n",
    "fdata=pd.read_csv('database_filled.csv',encoding=\"gbk\")\n",
    "raw_data=fdata.loc[:,[\n",
    "                     'Relative Atomic Mass',#0\n",
    "                      'Atomic Number',#1\n",
    "                      'Ionization Potential',#2\n",
    "                      'Electronegativity',#3\n",
    "                      'Number of d electrons',#4\n",
    "                      'ZIF or MOF Derived',#5\n",
    "                      'Hard or Soft Templated',#6\n",
    "                      'Graphene/Carbon Nanosheets or other 2D Structures',#7\n",
    "                      'Polymer Derived',#8\n",
    "                      'Carbon Nanofiber/Nanotubes',#9\n",
    "                      'Carbon Black Derived',#10\n",
    "                      'Biomass or other Organic Derived',#11  \n",
    "                      'Main Transition Metal Content (wt. %)',#12\n",
    "                      'Nitrogen Cotent (wt. %)',#13\n",
    "                      'Metal-N Coordination Number (XAS)',#14    \n",
    "                      'Pyridinic N Ratio',#15\n",
    "                      'Pyrrolic N Ratio',#16\n",
    "                      'Graphitic N Ratio',#17\n",
    "                      'Oxidized N Ratio',#18\n",
    "                      'Raman ID/IG Ratio',#19\n",
    "                      'BET Surface Area (m2/g)',#20\n",
    "                      'Acid Wash',#21\n",
    "                      'Pyrolysis Temperature (°C)',#22\n",
    "                      'Pyrolysis Time (h)',#23\n",
    "                      'Rising Rate (°C min-1)',#24\n",
    "                      'Flow Cell/H-type Cell',#25\n",
    "                      'Electrolyte Concentration (M)',#26\n",
    "                      'Catalyst Loading (mg cm-2)',#27\n",
    "                      'Nafion Membrane Thickness (μm)',#28\n",
    "                      'Carbon Paper/Glassy Carbon',#29\n",
    "                      'Electrode Area cm2',#30\n",
    "                      'Electrolyte pH'#31\n",
    "                        ]]\n",
    "###########defining a wrapper function for later call from each machine learning algorithms##########\n",
    "raw_input=raw_data.iloc[:,0:32]\n",
    "# X=raw_input.values.astype(np.float32)\n",
    "###########wrap up fuction for later call for OPTIMIZATION##########\n",
    "def evaluate(pre_2,real_2):\n",
    "    pre_2=np.array(pre_2)\n",
    "    real_2=np.array(real_2)\n",
    "    pre_2_series=pd.Series(pre_2)\n",
    "    real_2_series=pd.Series(real_2)\n",
    "    return rmse(pre_2,real_2), round(pre_2_series.corr(real_2_series), 3)\n",
    "def compare(list_name,limit):\n",
    "    judge=1\n",
    "    for a in list_name:\n",
    "        if a < limit:\n",
    "            judge=judge*1\n",
    "        else:\n",
    "            judge=judge*0\n",
    "    return judge\n",
    "def generate_arrays_from_file(path):\n",
    "    while True:\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                # create numpy arrays of input data\n",
    "                # and labels, from each line in the file\n",
    "                x1, x2, y = process_line(line)\n",
    "                yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
    "def Get_Average(list):\n",
    "    sum = 0\n",
    "    for item in list:     \n",
    "        sum += item  \n",
    "    return sum/len(list)\n",
    "def qualified_count(list_result,standard):\n",
    "    count = 0\n",
    "    for item in list_result:\n",
    "        if item > standard:\n",
    "            count+=1\n",
    "    return count\n",
    "def avg_top_x(list_result,top_number):\n",
    "    new_list=list_result[:]\n",
    "    new_list.sort(reverse=True)\n",
    "    return Get_Average(new_list[0:top_number])\n",
    "\n",
    "CD_05=fdata.loc[:,['FE of Product (CO) at -0.5V (vs.RHE)']]\n",
    "CD_06=fdata.loc[:,['FE of Product (CO) at -0.6V (vs.RHE)']]\n",
    "CD_07=fdata.loc[:,['FE of Product (CO) at -0.7V (vs.RHE)']]\n",
    "CD_08=fdata.loc[:,['FE of Product (CO) at -0.8V (vs.RHE)']]\n",
    "CD_09=fdata.loc[:,['FE of Product (CO) at -0.9V (vs.RHE)']]\n",
    "def gridsearch(model,param,algorithm_name,X_train,X_test,y_train,y_test):\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5,n_jobs=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    result = best_model.predict(X_test)\n",
    "    x_prediction_07=result\n",
    "    y_real_07=y_test.values\n",
    "    x_prediction_07_series=pd.Series(x_prediction_07)\n",
    "    y_real_07_series=pd.Series(y_real_07[:,0])   \n",
    "    result_train = best_model.predict(X_train)\n",
    "    x_prediction_07_train=result_train\n",
    "    y_real_07_train=y_train.values\n",
    "    x_prediction_07_series_train=pd.Series(x_prediction_07_train)\n",
    "    y_real_07_series_train=pd.Series(y_real_07_train[:,0])\n",
    "    ###########evaluating the regression quality##########\n",
    "    corr_ann = round(x_prediction_07_series.corr(y_real_07_series), 5)\n",
    "    error_val= compute_mae_mse_rmse(x_prediction_07,y_real_07[:,0])    \n",
    "    corr_ann_train = round(x_prediction_07_series_train.corr(y_real_07_series_train), 5)\n",
    "    error_val_train= compute_mae_mse_rmse(x_prediction_07_train,y_real_07_train[:,0])\n",
    "    return_list=[]\n",
    "    return_list.append(x_prediction_07)\n",
    "    return_list.append(y_real_07[:,0])\n",
    "    return return_list\n",
    "def compute_curves(prediction_list,real_list,algorithm_name):\n",
    "    for i in range(0,len(prediction_list[0])):\n",
    "        prediction_curve=[prediction_list[0][i],prediction_list[1][i],prediction_list[2][i],prediction_list[3][i],prediction_list[4][i]]\n",
    "        real_curve=[real_list[0][i],real_list[1][i],real_list[2][i],real_list[3][i],real_list[4][i]]\n",
    "        x_list=[0.5,0.6,0.7,0.8,0.9]\n",
    "        prediction_curve_series=pd.Series(prediction_curve)\n",
    "        real_curve_series=pd.Series(real_curve)\n",
    "        corr_ann = round(prediction_curve_series.corr(real_curve_series), 5)\n",
    "        error_val= compute_mae_mse_rmse(prediction_curve,real_curve)\n",
    "        \n",
    "        fig=plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(x_list,prediction_curve,label=algorithm_name+'Prediction',color='red')\n",
    "        ax.plot(x_list,real_curve,label='Real Curve',color='blue')\n",
    "        plt.legend()\n",
    "        plt.xlabel(u\"Potential V (vs. RHE)\")\n",
    "        plt.ylabel(u\"Faradaic Efficiency (%)\")\n",
    "        plt.savefig('ML FE CURVE %s th %s opt.png' %(i,algorithm_name))\n",
    "        print(algorithm_name)\n",
    "        print(corr_ann, error_val)\n",
    "def computing_different_algorithm(model,param,algorithm_name):\n",
    "    MODEL05=gridsearch(model,param,algorithm_name,X_train, X_test, y_train_05, y_test_05)\n",
    "    MODEL06=gridsearch(model,param,algorithm_name,X_train, X_test, y_train_06, y_test_06)\n",
    "    MODEL07=gridsearch(model,param,algorithm_name,X_train, X_test, y_train_07, y_test_07)\n",
    "    MODEL08=gridsearch(model,param,algorithm_name,X_train, X_test, y_train_08, y_test_08)\n",
    "    MODEL09=gridsearch(model,param,algorithm_name,X_train, X_test, y_train_09, y_test_09)    \n",
    "    prediction_list=[MODEL05[0],MODEL06[0],MODEL07[0],MODEL08[0],MODEL09[0]]\n",
    "    real_list=[MODEL05[1],MODEL06[1],MODEL07[1],MODEL08[1],MODEL09[1]]\n",
    "    compute_curves(prediction_list,real_list,algorithm_name)\n",
    "    print('finished')\n",
    "\n",
    "##########CatBoost gridsearch CV for best hyperparameter##########\n",
    "model_CatRegressor=catboost.CatBoostRegressor(random_state=1,verbose=0)\n",
    "param_cat = {\n",
    "'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5],\n",
    "'n_estimators':[50,100,200,400],\n",
    "'max_depth':[5,7,9,11],\n",
    "'subsample':[0.4,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "\n",
    "##########LGBM gridsearch CV for best hyperparameter##########\n",
    "model_LightGBMRegressor=lightgbm.LGBMRegressor(random_state=1,verbose=-1)\n",
    "param_light = {\n",
    "'boosting_type':['gbdt','rf'],\n",
    "'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5],\n",
    "'subsample':[0.4,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "'n_estimators':[50,100,200,400,None],\n",
    "'max_depth':[5,7,9,11,-1],\n",
    "'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    "'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "\n",
    " #########XGBoost gridsearch CV for best hyperparameter##########\n",
    "model_XGBRegressor=xgboost.XGBRegressor(objective='reg:squarederror',random_state=1,verbosity=0)\n",
    "param_xg = {\n",
    "'booster':['gbtree'],\n",
    "'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5],\n",
    "'n_estimators':[50,100,200,400,None],\n",
    "'max_depth':[5,7,9,11,16],\n",
    "'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    "'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "\n",
    "###########GradientBoost gridsearch CV for best hyperparameter##########\n",
    "model_GradientBoostingRegressor = ensemble.GradientBoostingRegressor(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_GB = {\n",
    "'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5],\n",
    "'criterion':['friedman_mse','mae','mse'],\n",
    "'max_features':['auto','sqrt','log2'],\n",
    "'loss':['ls', 'lad', 'huber', 'quantile']\n",
    "}\n",
    "\n",
    "###########RandomForest gridsearch CV for best hyperparameter##########\n",
    "model_RandomForestRegressor = ensemble.RandomForestRegressor(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_RF = {\n",
    "'n_estimators':[10,50,100,200,400],\n",
    "'max_depth':[3,5,7,9,11,None],\n",
    "'criterion':['mse','mae'],\n",
    "'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "\n",
    "###########Extra Tree gridsearch CV for best hyperparameter##########\n",
    "model_ExtraTreeRegressor = ExtraTreeRegressor(random_state=1)\n",
    "param_ET = {\n",
    "'max_depth':[5,6,7,8,9,10,11,None],\n",
    "'max_features':['auto','sqrt','log2'],\n",
    "'criterion' : [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "'splitter' : [ \"best\",'random']\n",
    "}\n",
    "\n",
    "###########Decision Tree gridsearch CV for best hyperparameter##########\n",
    "model_DecisionTreeRegressor = tree.DecisionTreeRegressor(random_state=1)\n",
    "param_DT = {\n",
    "'max_depth':[5,6,7,8,9,10,11,None],\n",
    "'max_features':['auto','sqrt','log2'],\n",
    "'criterion' : [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "'splitter' : [ \"best\",'random']\n",
    "}\n",
    "\n",
    "###########AdaBoost gridsearch CV for best hyperparameter##########\n",
    "model_AdaBoostRegressor = ensemble.AdaBoostRegressor(random_state=1)\n",
    "param_Ada = {\n",
    "'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5],\n",
    "'n_estimators':[50,100,200],\n",
    "'loss':['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "seed=8461\n",
    "X_train, X_test, y_train_05, y_test_05 = train_test_split(raw_input, CD_05, test_size=.012,random_state=seed)\n",
    "X_train, X_test, y_train_06, y_test_06 = train_test_split(raw_input, CD_06, test_size=.012,random_state=seed)\n",
    "X_train, X_test, y_train_07, y_test_07 = train_test_split(raw_input, CD_07, test_size=.012,random_state=seed)\n",
    "X_train, X_test, y_train_08, y_test_08 = train_test_split(raw_input, CD_08, test_size=.012,random_state=seed)\n",
    "X_train, X_test, y_train_09, y_test_09 = train_test_split(raw_input, CD_09, test_size=.012,random_state=seed)\n",
    "\n",
    "computing_different_algorithm(model_LightGBMRegressor,param_light,'LightGBM')\n",
    "computing_different_algorithm(model_XGBRegressor,param_xg,'XGBoost')\n",
    "computing_different_algorithm(model_CatRegressor,param_cat,'CatBoost')\n",
    "computing_different_algorithm(model_GradientBoostingRegressor,param_GB,'GradientBoost')\n",
    "computing_different_algorithm(model_RandomForestRegressor,param_RF,'Random Forest')\n",
    "computing_different_algorithm(model_ExtraTreeRegressor,param_ET,'Extra Tree')\n",
    "computing_different_algorithm(model_DecisionTreeRegressor,param_DT,'Decision Tree')\n",
    "computing_different_algorithm(model_AdaBoostRegressor,param_Ada,'AdaBoost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
