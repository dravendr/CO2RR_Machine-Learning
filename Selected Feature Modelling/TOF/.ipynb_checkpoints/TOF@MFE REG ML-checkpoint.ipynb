{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "###########import packages##########\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout \n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.constraints import maxnorm \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost \n",
    "import lightgbm\n",
    "import catboost\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import ensemble\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "%matplotlib \n",
    "import shap\n",
    "from catboost import EFstrType, Pool\n",
    "###########wrapping root mean square error for later calls##########\n",
    "def compute_mae_mse_rmse(target,prediction):\n",
    "    error = []\n",
    "    for i in range(len(target)):\n",
    "        error.append(target[i] - prediction[i])\n",
    "    squaredError = []\n",
    "    absError = []\n",
    "    for val in error:\n",
    "        squaredError.append(val * val)  # target-prediction之差平方\n",
    "        absError.append(abs(val))  # 误差绝对值\n",
    "    mae=sum(absError)/len(absError)  # 平均绝对误差MAE\n",
    "    mse=sum(squaredError)/len(squaredError)  # 均方误差MSE\n",
    "    RMSE=np.sqrt(sum(squaredError)/len(squaredError))\n",
    "    R2=r2_score(target,prediction)\n",
    "    return mae,mse,RMSE,R2\n",
    "###########loading data##########\n",
    "fdata=pd.read_csv('database_filled_TOF_MCL.csv',encoding=\"gbk\")\n",
    "raw_data=fdata.loc[:,[\n",
    "                     'Ionization Potential',#0\n",
    "                      'Electronegativity',#1\n",
    "                      'Number of d electrons',#2\n",
    "                      'Graphene/Carbon Nanosheets or other 2D Structures',#3\n",
    "                      'Carbon Nanofiber/Nanotubes',#4\n",
    "                      'Biomass or other Organic Derived',#5  \n",
    "                      'Main Transition Metal Content (wt. %)',#6\n",
    "                      'Nitrogen Cotent (wt. %)',#7\n",
    "                      'Metal-N Coordination Number (XAS)',#8    \n",
    "                      'Pyridinic N Ratio',#9\n",
    "                      'Pyrrolic N Ratio',#10\n",
    "                      'Raman ID/IG Ratio',#11\n",
    "                      'BET Surface Area (m2/g)',#12\n",
    "                      'Pyrolysis Temperature (°C)',#13\n",
    "                      'Pyrolysis Time (h)',#14\n",
    "                      'Rising Rate (°C min-1)',#15\n",
    "                      'Flow Cell/H-type Cell',#16\n",
    "                      'Electrolyte Concentration (M)',#17\n",
    "                      'Catalyst Loading (mg cm-2)',#18\n",
    "                      'Carbon Paper/Glassy Carbon',#19\n",
    "                      'Electrolyte pH',#20\n",
    "                      'TOF@Maximum FE(h-1) (log)'#21\n",
    "                        ]]\n",
    "\n",
    "\n",
    "\n",
    "###########defining a wrapper function for later call from each machine learning algorithms##########\n",
    "raw_input=raw_data.iloc[:,0:21]\n",
    "raw_output=raw_data.iloc[:,21]\n",
    "# raw_input_global=raw_data.iloc[:,0:32]\n",
    "# raw_output_global=raw_data.iloc[:,32]\n",
    "X=raw_input.values.astype(np.float32)\n",
    "y=raw_output.values.astype(np.float32)\n",
    "###########wrap up fuction for later call for OPTIMIZATION##########\n",
    "def evaluate(pre_2,real_2):\n",
    "    pre_2=np.array(pre_2)\n",
    "    real_2=np.array(real_2)\n",
    "    pre_2_series=pd.Series(pre_2)\n",
    "    real_2_series=pd.Series(real_2)\n",
    "    return rmse(pre_2,real_2), round(pre_2_series.corr(real_2_series), 3)\n",
    "def compare(list_name,limit):\n",
    "    judge=1\n",
    "    for a in list_name:\n",
    "        if a < limit:\n",
    "            judge=judge*1\n",
    "        else:\n",
    "            judge=judge*0\n",
    "    return judge\n",
    "def generate_arrays_from_file(path):\n",
    "    while True:\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                # create numpy arrays of input data\n",
    "                # and labels, from each line in the file\n",
    "                x1, x2, y = process_line(line)\n",
    "                yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(model,param,algorithm_name):\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5,n_jobs=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    result = best_model.predict(X_test)\n",
    "    x_prediction_07=result\n",
    "    y_real_07=y_test.values\n",
    "    x_prediction_07_series=pd.Series(x_prediction_07)\n",
    "    y_real_07_series=pd.Series(y_real_07)\n",
    "    \n",
    "    result_train = best_model.predict(X_train)\n",
    "    x_prediction_07_train=result_train\n",
    "    y_real_07_train=y_train.values\n",
    "    x_prediction_07_series_train=pd.Series(x_prediction_07_train)\n",
    "    y_real_07_series_train=pd.Series(y_real_07_train)\n",
    "    \n",
    "    ###########evaluating the regression quality##########\n",
    "    corr_ann = round(x_prediction_07_series.corr(y_real_07_series), 5)\n",
    "    error_val= compute_mae_mse_rmse(x_prediction_07,y_real_07)\n",
    "    \n",
    "    corr_ann_train = round(x_prediction_07_series_train.corr(y_real_07_series_train), 5)\n",
    "    error_val_train= compute_mae_mse_rmse(x_prediction_07_train,y_real_07_train)\n",
    "    \n",
    "    print(algorithm_name)\n",
    "    print(best_model.feature_importances_)\n",
    "    print('Best Regressor:',grid.best_params_,'Best Score:', grid.best_score_)\n",
    "    print(error_val,'TEST R2',error_val[3],'TEST CORR',corr_ann)\n",
    "    print(error_val_train,'TRAIN R2',error_val_train[3],'TRAIN CORR',corr_ann_train)\n",
    "    x_y_x=np.arange(0,5,0.1)\n",
    "    x_y_y=np.arange(0,5,0.1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x_prediction_07,y_real_07,color='red',label=algorithm_name+' Test Set',alpha=0.75)\n",
    "    ax.scatter(x_prediction_07_train,y_real_07_train,color='blue',label=algorithm_name+' Training Set',alpha=0.25,marker=\"^\")\n",
    "    ax.plot(x_y_x,x_y_y)\n",
    "    plt.legend()\n",
    "    plt.xlabel(u\"Predicted_Log(TOF(h-1))@_Maximum_Faradaic_Efficiency\")\n",
    "    plt.ylabel(u\"Real_Log(TOF(h-1))@_Maximum_Faradaic_Efficiency\")\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import *\n",
    "def shap_plot(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:21]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,21]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        shap_values=shap_values[:,:-1]\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values).mean(0)\n",
    "        print(global_importances)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values).mean(0)\n",
    "        print(global_importances)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values).mean(0)\n",
    "        print(global_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=9\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_input, raw_output, test_size=.1,random_state=seed)\n",
    "# ##########CatBoost gridsearch CV for best hyperparameter##########\n",
    "# model_CatRegressor=catboost.CatBoostRegressor(random_state=1,verbose=0)\n",
    "# param_cat = {\n",
    "#  'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "#  'n_estimators':[50,100,200,400],\n",
    "#  'max_depth':[5,7,9,11],\n",
    "#  \"boosting_type\":[\"Plain\"],\n",
    "#  'subsample':[0.4,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "#     'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "# }\n",
    "# gridsearch(model_CatRegressor,param_cat,'CatBoost')\n",
    "# ##########LGBM gridsearch CV for best hyperparameter##########\n",
    "# model_LightGBMRegressor=lightgbm.LGBMRegressor(random_state=1,verbose=-1)\n",
    "# param_light = {\n",
    "#  'boosting_type':['gbdt','rf'],\n",
    "#     'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "#     'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "#      'n_estimators':[50,100,200,400],\n",
    "#     'max_depth':[5,7,9,11,-1],\n",
    "#  'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    "#  'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "# }\n",
    "# gridsearch(model_LightGBMRegressor,param_light,'LightGBM')\n",
    "\n",
    "# #########XGBoost gridsearch CV for best hyperparameter##########\n",
    "# model_XGBRegressor=xgboost.XGBRegressor(objective='reg:squarederror',random_state=1,verbosity=0)\n",
    "# param_xg = {\n",
    "#  'booster':['gbtree'],\n",
    "#   'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "#  'n_estimators':[50,100,200,400],\n",
    "#  'max_depth':[5,7,9,11],\n",
    "#  'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "#  'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    "#  'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "# }\n",
    "# gridsearch(model_XGBRegressor,param_xg,'XGBoost')\n",
    "\n",
    "# ###########GradientBoost gridsearch CV for best hyperparameter##########\n",
    "# model_GradientBoostingRegressor = ensemble.GradientBoostingRegressor(random_state=1)\n",
    "# ###########defining the parameters dictionary##########\n",
    "# param_GB = {\n",
    "#  'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "#                 'criterion':['friedman_mse','mae','mse'],\n",
    "#                  'max_features':['auto','sqrt','log2'],\n",
    "#                  'loss':['ls', 'lad', 'huber', 'quantile'],\n",
    "#     'max_depth':[3,5,7,9,11,13,16]\n",
    "# }\n",
    "# gridsearch(model_GradientBoostingRegressor,param_GB,'GradientBoost')\n",
    "\n",
    "# ###########RandomForest gridsearch CV for best hyperparameter##########\n",
    "# model_RandomForestRegressor = ensemble.RandomForestRegressor(random_state=1)\n",
    "# ###########defining the parameters dictionary##########\n",
    "# param_RF = {\n",
    "#             'criterion':['mse','mae'],\n",
    "#             'max_features':['auto','sqrt','log2'],\n",
    "#     'n_estimators':[10,50,100,200,400],\n",
    "#      'max_depth':[3,5,7,9,11,None]\n",
    "# }\n",
    "# gridsearch(model_RandomForestRegressor,param_RF,'Random Forest')\n",
    "\n",
    "\n",
    "# ###########Extra Tree gridsearch CV for best hyperparameter##########\n",
    "# model_ExtraTreeRegressor = ExtraTreeRegressor(random_state=1)\n",
    "# param_ET = {\n",
    "#         'max_depth':[5,6,7,8,9,10,11,None],\n",
    "#                'max_features':['auto','sqrt','log2'],\n",
    "#                'criterion' : [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "#                'splitter' : [ \"best\",'random']\n",
    "# }\n",
    "# gridsearch(model_ExtraTreeRegressor,param_ET,'Extra Tree')\n",
    "\n",
    "\n",
    "# ###########Decision Tree gridsearch CV for best hyperparameter##########\n",
    "# model_DecisionTreeRegressor = tree.DecisionTreeRegressor(random_state=1)\n",
    "# param_DT = {\n",
    "#         'max_depth':[5,6,7,8,9,10,11,None],\n",
    "#                'max_features':['auto','sqrt','log2'],\n",
    "#                'criterion' : [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "#                'splitter' : [ \"best\",'random']\n",
    "# }\n",
    "# gridsearch(model_DecisionTreeRegressor,param_DT,'Decision Tree')\n",
    "\n",
    "\n",
    "# ###########AdaBoost gridsearch CV for best hyperparameter##########\n",
    "# model_AdaBoostRegressor = ensemble.AdaBoostRegressor(random_state=1)\n",
    "# param_Ada = {\n",
    "#      'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "#     'n_estimators':[50,100,200],\n",
    "#             'loss':['linear', 'square', 'exponential']\n",
    "# }\n",
    "# gridsearch(model_AdaBoostRegressor,param_Ada,'AdaBoost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_plot_interaction_all(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:32]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,32]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    print('train finished')\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        shap_values=shap_values[:,:-1]\n",
    "        \n",
    "#         shap_values = model.get_feature_importance(Pool(data[Xtrain.columns]),type=EFstrType.ShapValues,verbose=100)\n",
    "        interaction_values = best_model.get_feature_importance(Pool(SHAP_INPUT),type=EFstrType.ShapInteractionValues)\n",
    "        interaction_values=interaction_values[:,:-1,:-1]\n",
    "        shap.summary_plot(interaction_values,SHAP_INPUT, max_display=10)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,SHAP_INPUT)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)        \n",
    "        interaction_values = shap.TreeExplainer(best_model).shap_interaction_values(SHAP_INPUT)\n",
    "        shap.summary_plot(interaction_values,SHAP_INPUT, max_display=10)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,SHAP_INPUT)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        interaction_values = shap.TreeExplainer(best_model).shap_interaction_values(SHAP_INPUT)\n",
    "        shap.summary_plot(interaction_values, SHAP_INPUT, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n",
      "[ 23  56  55  14  10   7 297 287 108 248 272 260 225  91  44 136  42  28\n",
      " 119  15  41]\n",
      "Best Regressor: {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.001, 'reg_lambda': 0.001, 'subsample': 0.4} Best Score: 0.6542740141447381\n",
      "(0.3149690077301405, 0.14386861539242218, 0.37930016529448307, 0.7441786063021614) TEST R2 0.7441786063021614 TEST CORR 0.90044\n",
      "(0.08878657651376348, 0.013466451961374599, 0.11604504281258463, 0.9804997984087266) TRAIN R2 0.9804997984087266 TRAIN CORR 0.9915\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "##########LGBM gridsearch CV for best hyperparameter##########\n",
    "model_LightGBMRegressor=lightgbm.LGBMRegressor(random_state=1,verbose=-1)\n",
    "param_light = {\n",
    " 'boosting_type':['gbdt'],\n",
    " 'learning_rate':[0.1],\n",
    " 'n_estimators':[200],\n",
    "    'max_depth':[7],\n",
    " 'subsample':[0.4],\n",
    " 'reg_alpha':[0.001],\n",
    " 'reg_lambda':[0.001]\n",
    "}\n",
    "gridsearch(model_LightGBMRegressor,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n",
      "[0.02014456 0.20205689 0.05171231 0.01631113 0.00953529 0.00714566\n",
      " 0.41247442 0.06800645 0.06349643 0.0618285  0.09122622 0.04142286\n",
      " 0.06564696 0.05625982 0.03459385 0.04728925 0.02928494 0.00598944\n",
      " 0.23090075 0.02828157 0.03138106]\n"
     ]
    }
   ],
   "source": [
    "shap_plot(model_LightGBMRegressor,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "[0.01246162 0.16504107 0.04669    0.01048189 0.01039817 0.01151408\n",
      " 0.06444213 0.02772197 0.04770399 0.0315632  0.03226065 0.03038094\n",
      " 0.03395351 0.06150754 0.0438227  0.03310884 0.10184143 0.02122563\n",
      " 0.10736505 0.0432806  0.063235  ]\n",
      "Best Regressor: {'booster': 'gbtree', 'learning_rate': 0.025, 'max_depth': 9, 'n_estimators': 400, 'reg_alpha': 0.001, 'reg_lambda': 0.001, 'subsample': 0.4} Best Score: 0.6904997772493175\n",
      "(0.26227189285058156, 0.10957768399312388, 0.3310252014471464, 0.8054320514341314) TEST R2 0.8054320514341314 TEST CORR 0.9222\n",
      "(0.03333999775295492, 0.0016258658302823726, 0.040322026614275884, 0.9977594445460037) TRAIN R2 0.9977594445460037 TRAIN CORR 0.99903\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "##########XGBoost gridsearch CV for best hyperparameter##########\n",
    "model_XGBRegressor=xgboost.XGBRegressor(objective='reg:squarederror',random_state=1,verbose=0)\n",
    "param_xg = {\n",
    " 'booster':['gbtree'],\n",
    " 'learning_rate':[0.025],\n",
    " 'n_estimators':[400],\n",
    " 'max_depth':[9],\n",
    " 'subsample':[0.4],\n",
    " 'reg_alpha':[0.001],\n",
    " 'reg_lambda':[0.001]\n",
    "}\n",
    "gridsearch(model_XGBRegressor,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "[0.07042809 0.17280284 0.03737342 0.00998343 0.01392586 0.01364009\n",
      " 0.39100382 0.06178574 0.0479683  0.05730871 0.06619222 0.02977084\n",
      " 0.03207129 0.06635255 0.01988114 0.02496746 0.01902883 0.00844679\n",
      " 0.21181511 0.01361194 0.03386283]\n"
     ]
    }
   ],
   "source": [
    "shap_plot(model_XGBRegressor,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n",
      "[ 3.87717162  7.01566223  4.67385658  0.60080676  1.07660463  0.49733489\n",
      " 27.13197667  5.53989331  5.82864127  3.17755246  4.59559809  4.76862367\n",
      "  2.91677753  4.58607786  2.05464599  3.29795006  3.43673963  1.12960117\n",
      " 11.31252971  0.16108307  2.32087281]\n",
      "Best Regressor: {'learning_rate': 0.075, 'max_depth': 5, 'n_estimators': 400, 'reg_lambda': 1e-05, 'subsample': 0.4} Best Score: 0.6920988632530778\n",
      "(0.26148353787319023, 0.10756273808561896, 0.327967586943617, 0.8137686362404637) TEST R2 0.8137686362404637 TEST CORR 0.92896\n",
      "(0.08297869498265713, 0.010709532692966001, 0.10348687208030785, 0.9844966288398126) TRAIN R2 0.9844966288398126 TRAIN CORR 0.99341\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "##########CatBoost gridsearch CV for best hyperparameter##########\n",
    "model_CatRegressor=catboost.CatBoostRegressor(random_state=1,verbose=0)\n",
    "param_cat = {\n",
    " 'learning_rate':[0.075],\n",
    " 'n_estimators':[400],\n",
    " 'max_depth':[5],\n",
    " 'subsample':[0.4],\n",
    "    'reg_lambda':[1e-05]\n",
    "}\n",
    "gridsearch(model_CatRegressor,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n",
      "[0.07209101 0.13013997 0.06450445 0.01244462 0.01716233 0.01665764\n",
      " 0.35703721 0.05867437 0.06693106 0.04949466 0.0600782  0.04235546\n",
      " 0.0329057  0.07913192 0.02418668 0.03559048 0.05185716 0.01386427\n",
      " 0.19870582 0.00438217 0.0267715 ]\n"
     ]
    }
   ],
   "source": [
    "shap_plot(model_CatRegressor,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost\n",
      "[0.0292902  0.05488004 0.03270973 0.00753018 0.00902487 0.00190927\n",
      " 0.28439501 0.07645863 0.05497649 0.03899556 0.03033551 0.04771914\n",
      " 0.03929884 0.0558049  0.01263168 0.02521267 0.03119687 0.00808295\n",
      " 0.11899613 0.00969825 0.03085307]\n",
      "Best Regressor: {'criterion': 'mse', 'learning_rate': 0.1, 'loss': 'huber', 'max_depth': 5, 'max_features': 'sqrt'} Best Score: 0.6455618619422293\n",
      "(0.28300800957079875, 0.14248622085015872, 0.3774734703925015, 0.6896236465189054) TEST R2 0.6896236465189054 TEST CORR 0.89893\n",
      "(0.09530171517482651, 0.020487806234000847, 0.14313562182070838, 0.9687270290759493) TRAIN R2 0.9687270290759493 TRAIN CORR 0.98768\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "###########GradientBoost gridsearch CV for best hyperparameter##########\n",
    "model_GradientBoostingRegressor = ensemble.GradientBoostingRegressor(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_GB = {\n",
    " 'learning_rate':[0.1],\n",
    "                'criterion':['mse'],\n",
    "                 'max_features':['sqrt'],\n",
    "                 'loss':['huber'],\n",
    "    'max_depth':[5]\n",
    "}\n",
    "gridsearch(model_GradientBoostingRegressor,param_GB,'GradientBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost\n",
      "[0.0588181  0.1407012  0.06517526 0.01401971 0.02666322 0.01522485\n",
      " 0.31862146 0.04761229 0.04780775 0.04314466 0.03533252 0.02862866\n",
      " 0.02848819 0.07709878 0.02334955 0.02202986 0.02661915 0.01146137\n",
      " 0.19182702 0.01487362 0.02733467]\n"
     ]
    }
   ],
   "source": [
    "shap_plot(model_GradientBoostingRegressor,param_GB,'GradientBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "[0.0226564  0.09667016 0.02432655 0.00327218 0.00604694 0.00210368\n",
      " 0.38536195 0.04938076 0.03111166 0.0490697  0.03632575 0.0326772\n",
      " 0.03256537 0.03826828 0.01284387 0.01809601 0.01939995 0.00475193\n",
      " 0.11451709 0.00692322 0.01363137]\n",
      "Best Regressor: {'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'n_estimators': 400} Best Score: 0.6213107781700922\n",
      "(0.29090116333474086, 0.13248151967234062, 0.3639801088965448, 0.7003177286014094) TEST R2 0.7003177286014094 TEST CORR 0.90887\n",
      "(0.14499827569422946, 0.03707435289670253, 0.19254701476964667, 0.9337765825746714) TRAIN R2 0.9337765825746714 TRAIN CORR 0.98212\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "###########RandomForest gridsearch CV for best hyperparameter##########\n",
    "model_RandomForestRegressor = ensemble.RandomForestRegressor(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_RF = {\n",
    "    'n_estimators':[400],\n",
    "     'max_depth':[None],\n",
    "            'criterion':['mse'],\n",
    "            'max_features':['auto']\n",
    "}\n",
    "gridsearch(model_RandomForestRegressor,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|=================== | 464/482 [00:26<00:01]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01868852 0.2066701  0.02979614 0.00338588 0.01160301 0.00385072\n",
      " 0.39025333 0.02741355 0.02309394 0.03487622 0.03191887 0.01575315\n",
      " 0.01678976 0.04644411 0.00657692 0.01086461 0.01581299 0.00348492\n",
      " 0.20681438 0.0091299  0.0114954 ]\n"
     ]
    }
   ],
   "source": [
    "shap_plot(model_RandomForestRegressor,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree\n",
      "[0.01181752 0.12396338 0.02985324 0.01836826 0.01105416 0.\n",
      " 0.38787097 0.         0.02021933 0.01206045 0.02707305 0.01622411\n",
      " 0.0083206  0.08707075 0.         0.01923885 0.03051363 0.00423381\n",
      " 0.19211788 0.         0.        ]\n",
      "Best Regressor: {'criterion': 'mae', 'max_depth': 5, 'max_features': 'auto', 'splitter': 'best'} Best Score: 0.36208652828223686\n",
      "(0.4114808775, 0.28680500845946755, 0.5355417896480792, 0.363955099618969) TEST R2 0.363955099618969 TEST CORR 0.76011\n",
      "(0.35023591436720536, 0.2439776660091815, 0.4939409539703926, 0.5544756564335704) TRAIN R2 0.5544756564335704 TRAIN CORR 0.82278\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "###########Extra Tree gridsearch CV for best hyperparameter##########\n",
    "model_ExtraTreeRegressor = ExtraTreeRegressor(random_state=1)\n",
    "param_ET = {\n",
    "        'max_depth':[5],\n",
    "               'max_features':['auto'],\n",
    "               'criterion' : [ \"mae\"],\n",
    "               'splitter' : [ 'best']\n",
    "}\n",
    "gridsearch(model_ExtraTreeRegressor,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree\n",
      "[0.00302089 0.21175454 0.0223236  0.01207509 0.0094525  0.\n",
      " 0.38345403 0.         0.01219838 0.00770858 0.04023651 0.00979351\n",
      " 0.00851961 0.10093142 0.         0.00830756 0.01245656 0.00129743\n",
      " 0.29085629 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "shap_plot(model_ExtraTreeRegressor,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "[0.01181752 0.12396338 0.02985324 0.01836826 0.01105416 0.\n",
      " 0.38787097 0.         0.02021933 0.01206045 0.02707305 0.01622411\n",
      " 0.0083206  0.08707075 0.         0.01923885 0.03051363 0.00423381\n",
      " 0.19211788 0.         0.        ]\n",
      "Best Regressor: {'criterion': 'mae', 'max_depth': 5, 'max_features': 'auto', 'splitter': 'best'} Best Score: 0.36208652828223686\n",
      "(0.4114808775, 0.28680500845946755, 0.5355417896480792, 0.363955099618969) TEST R2 0.363955099618969 TEST CORR 0.76011\n",
      "(0.35023591436720536, 0.2439776660091815, 0.4939409539703926, 0.5544756564335704) TRAIN R2 0.5544756564335704 TRAIN CORR 0.82278\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "###########Decision Tree gridsearch CV for best hyperparameter##########\n",
    "model_DecisionTreeRegressor = tree.DecisionTreeRegressor(random_state=1)\n",
    "param_DT = {\n",
    "        'max_depth':[5],\n",
    "               'max_features':['auto'],\n",
    "               'criterion' : [ \"mae\"],\n",
    "               'splitter' : [ 'best']\n",
    "}\n",
    "gridsearch(model_DecisionTreeRegressor,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "[0.00302089 0.21175454 0.0223236  0.01207509 0.0094525  0.\n",
      " 0.38345403 0.         0.01219838 0.00770858 0.04023651 0.00979351\n",
      " 0.00851961 0.10093142 0.         0.00830756 0.01245656 0.00129743\n",
      " 0.29085629 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "shap_plot(model_DecisionTreeRegressor,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n",
      "[0.01215778 0.11748358 0.02005311 0.00312003 0.00280304 0.00252277\n",
      " 0.40077515 0.06382648 0.0192254  0.04681902 0.03210632 0.01688543\n",
      " 0.01736189 0.02121552 0.02156688 0.02705356 0.04529005 0.00184151\n",
      " 0.08944725 0.00592023 0.03252502]\n",
      "Best Regressor: {'learning_rate': 1, 'loss': 'square', 'n_estimators': 100} Best Score: 0.54734380093252\n",
      "(0.37011339885782246, 0.1880812242101097, 0.4336833224947781, 0.5365590874936396) TEST R2 0.5365590874936396 TEST CORR 0.86164\n",
      "(0.3857423213559598, 0.2159091002542667, 0.46466019869821723, 0.4961425438249898) TRAIN R2 0.4961425438249898 TRAIN CORR 0.8491\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "###########AdaBoost gridsearch CV for best hyperparameter##########\n",
    "model_AdaBoostRegressor = ensemble.AdaBoostRegressor(random_state=1)\n",
    "param_Ada = {\n",
    "     'n_estimators':[100],\n",
    "     'learning_rate':[1],\n",
    "            'loss':[ 'square']\n",
    "}\n",
    "gridsearch(model_AdaBoostRegressor,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n",
      "[0.00684002 0.17763703 0.01063568 0.00278139 0.00379708 0.00476891\n",
      " 0.37021874 0.0283914  0.0176478  0.04188952 0.01734816 0.01193473\n",
      " 0.01369394 0.01504943 0.013657   0.01438121 0.01730572 0.00389973\n",
      " 0.11750061 0.00344588 0.03456713]\n"
     ]
    }
   ],
   "source": [
    "shap_plot(model_AdaBoostRegressor,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
