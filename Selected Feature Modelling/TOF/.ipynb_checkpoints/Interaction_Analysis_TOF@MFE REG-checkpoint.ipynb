{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########import packages##########\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn import linear_model\n",
    "import lightgbm\n",
    "import catboost\n",
    "import xgboost\n",
    "import shap\n",
    "from catboost import *\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from catboost import *\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "%matplotlib\n",
    "###########wrapping root mean square error for later calls##########\n",
    "def compute_mae_mse_rmse(target,prediction):\n",
    "    error = []\n",
    "    for i in range(len(target)):\n",
    "        error.append(target[i] - prediction[i])\n",
    "    squaredError = []\n",
    "    absError = []\n",
    "    for val in error:\n",
    "        squaredError.append(val * val)  # target-prediction之差平方\n",
    "        absError.append(abs(val))  # 误差绝对值\n",
    "    mae=sum(absError)/len(absError)  # 平均绝对误差MAE\n",
    "    mse=sum(squaredError)/len(squaredError)  # 均方误差MSE\n",
    "    RMSE=np.sqrt(sum(squaredError)/len(squaredError))\n",
    "    R2=r2_score(target,prediction)\n",
    "    return mae,mse,RMSE,R2\n",
    "###########loading data##########\n",
    "fdata=pd.read_csv('database_filled_TOF_MCL.csv',encoding=\"gbk\")\n",
    "raw_data=fdata.loc[:,[\n",
    "                       'Ionization Potential',#0\n",
    "                      'Electronegativity',#1\n",
    "                      'Number of d electrons',#2\n",
    "                      'Graphene/Carbon Nanosheets or other 2D Structures',#3\n",
    "                      'Carbon Nanofiber/Nanotubes',#4\n",
    "                      'Biomass or other Organic Derived',#5  \n",
    "                      'Main Transition Metal Content (wt. %)',#6\n",
    "                      'Nitrogen Cotent (wt. %)',#7\n",
    "                      'Metal-N Coordination Number (XAS)',#8    \n",
    "                      'Pyridinic N Ratio',#9\n",
    "                      'Pyrrolic N Ratio',#10\n",
    "                      'Raman ID/IG Ratio',#11\n",
    "                      'BET Surface Area (m2/g)',#12\n",
    "                      'Pyrolysis Temperature (°C)',#13\n",
    "                      'Pyrolysis Time (h)',#14\n",
    "                      'Rising Rate (°C min-1)',#15\n",
    "                      'Flow Cell/H-type Cell',#16\n",
    "                      'Electrolyte Concentration (M)',#17\n",
    "                      'Catalyst Loading (mg cm-2)',#18\n",
    "                      'Carbon Paper/Glassy Carbon',#19\n",
    "                      'Electrolyte pH',#20\n",
    "                      'TOF@Maximum FE(h-1) (log)'#21\n",
    "                        ]]\n",
    "###########defining a wrapper function for later call from each machine learning algorithms##########\n",
    "raw_param=raw_data.iloc[:,0:21]\n",
    "raw_power=raw_data.iloc[:,21]\n",
    "# raw_input_global=raw_data.iloc[:,0:32]\n",
    "# raw_output_global=raw_data.iloc[:,32]\n",
    "X=raw_param.values.astype(np.float32)\n",
    "y=raw_power.values.astype(np.float32)\n",
    "###########wrap up fuction for later call for OPTIMIZATION##########\n",
    "def evaluate(pre_2,real_2):\n",
    "    pre_2=np.array(pre_2)\n",
    "    real_2=np.array(real_2)\n",
    "    pre_2_series=pd.Series(pre_2)\n",
    "    real_2_series=pd.Series(real_2)\n",
    "    return rmse(pre_2,real_2), round(pre_2_series.corr(real_2_series), 3)\n",
    "def compare(list_name,limit):\n",
    "    judge=1\n",
    "    for a in list_name:\n",
    "        if a < limit:\n",
    "            judge=judge*1\n",
    "        else:\n",
    "            judge=judge*0\n",
    "    return judge\n",
    "def generate_arrays_from_file(path):\n",
    "    while True:\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                # create numpy arrays of input data\n",
    "                # and labels, from each line in the file\n",
    "                x1, x2, y = process_line(line)\n",
    "                yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def gridsearch(model,param,algorithm_name):\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5,n_jobs=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    result = best_model.predict(X_test)\n",
    "    x_prediction_07=result\n",
    "    y_real_07=y_test.values\n",
    "    x_prediction_07_series=pd.Series(x_prediction_07)\n",
    "    y_real_07_series=pd.Series(y_real_07)\n",
    "    \n",
    "    result_train = best_model.predict(X_train)\n",
    "    x_prediction_07_train=result_train\n",
    "    y_real_07_train=y_train.values\n",
    "    x_prediction_07_series_train=pd.Series(x_prediction_07_train)\n",
    "    y_real_07_series_train=pd.Series(y_real_07_train)\n",
    "    \n",
    "    ###########evaluating the regression quality##########\n",
    "    corr_ann = round(x_prediction_07_series.corr(y_real_07_series), 5)\n",
    "    error_val= compute_mae_mse_rmse(x_prediction_07,y_real_07)\n",
    "    \n",
    "    corr_ann_train = round(x_prediction_07_series_train.corr(y_real_07_series_train), 5)\n",
    "    error_val_train= compute_mae_mse_rmse(x_prediction_07_train,y_real_07_train)\n",
    "    \n",
    "    print(algorithm_name)\n",
    "    print(best_model.feature_importances_)\n",
    "    print('Best Regressor:',grid.best_params_,'Best Score:', grid.best_score_)\n",
    "    print(error_val,'TEST R2',error_val[3],'TEST CORR',corr_ann)\n",
    "    print(error_val_train,'TRAIN R2',error_val_train[3],'TRAIN CORR',corr_ann_train)\n",
    "    x_y_x=np.arange(0,5,0.1)\n",
    "    x_y_y=np.arange(0,5,0.1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x_prediction_07,y_real_07,color='red',label=algorithm_name+' Test Set',alpha=0.75)\n",
    "    ax.scatter(x_prediction_07_train,y_real_07_train,color='blue',label=algorithm_name+' Training Set',alpha=0.25,marker=\"^\")\n",
    "    ax.plot(x_y_x,x_y_y)\n",
    "    plt.legend()\n",
    "    plt.xlabel(u\"Predicted_Log(TOF(h-1))@_Maximum_Faradaic_Efficiency\")\n",
    "    plt.ylabel(u\"Real_Log(TOF(h-1))@_Maximum_Faradaic_Efficiency\")\n",
    "    print('finished')\n",
    "    return best_model\n",
    "def shap_plot_interaction(best_model,algorithm_name,interacted_features):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_param\n",
    "    SHAP_OUTPUT=raw_power\n",
    "    print('train finished')\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        shap_values=shap_values[:,:-1]\n",
    "        shap.dependence_plot(interacted_features[0], shap_values, SHAP_INPUT,interaction_index= interacted_features[1])\n",
    "        shap.dependence_plot(interacted_features[1], shap_values, SHAP_INPUT,interaction_index= interacted_features[0])\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,SHAP_INPUT)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)        \n",
    "        interaction_values = shap.TreeExplainer(best_model).shap_interaction_values(SHAP_INPUT) \n",
    "        print(shap_values)\n",
    "        shap.dependence_plot(interacted_features[0], shap_values, SHAP_INPUT,interaction_index= interacted_features[1])\n",
    "        shap.dependence_plot(interacted_features[1], shap_values, SHAP_INPUT,interaction_index= interacted_features[0])\n",
    "    elif algorithm_name=='ANN':\n",
    "        SHAP_INPUT=standardized_data.iloc[:,0:21]\n",
    "        SHAP_OUTPUT=raw_data.iloc[:,21]\n",
    "        X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "        y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "        explainer = shap.DeepExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP) \n",
    "        print(shap_values)\n",
    "        shap.dependence_plot(interacted_features[0], shap_values[0], SHAP_INPUT,interaction_index= interacted_features[1])\n",
    "        shap.dependence_plot(interacted_features[1], shap_values[0], SHAP_INPUT,interaction_index= interacted_features[0])\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,SHAP_INPUT)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        interaction_values = shap.TreeExplainer(best_model).shap_interaction_values(SHAP_INPUT)\n",
    "        shap.dependence_plot(interacted_features[0], shap_values, SHAP_INPUT,interaction_index= interacted_features[1])\n",
    "        shap.dependence_plot(interacted_features[1], shap_values, SHAP_INPUT,interaction_index= interacted_features[0])\n",
    "from pdpbox import pdp\n",
    "def plot_pdp_interact_ANN(model, df, f_list, cluster_flag=False, nb_clusters=None, lines_flag=False):\n",
    "    \n",
    "    # Create the data that we will plot\n",
    "    inter1 = pdp.pdp_interact(model, df, model_features=df.columns.tolist(), features=f_list,num_grid_points=[20,20])\n",
    "    # plot it\n",
    "    settings = {\n",
    "            'contour_color':  'white',\n",
    "            'font_family': 'Arial',\n",
    "            # matplotlib color map for interact plot\n",
    "            'cmap': 'viridis',\n",
    "            # fill alpha for interact plot\n",
    "            'inter_fill_alpha': 0.8,\n",
    "            # fontsize for interact plot text\n",
    "            'inter_fontsize': 7,\n",
    "        }\n",
    "    pdp.pdp_interact_plot(\n",
    "    pdp_interact_out=inter1, feature_names=f_list, plot_type='contour',figsize=(10,10),x_quantile=True, plot_pdp=True,plot_params=settings)\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.utils import validation\n",
    "def pdp_plot_2d(best_model,f_list):\n",
    "    print('start')\n",
    "    validation.check_is_fitted(estimator=best_model)\n",
    "    my_plots =plot_partial_dependence(best_model, features=[f_list], X=raw_param, percentiles=(0, 1),grid_resolution=100)\n",
    "def pdp_plot_2d_XG_CAT(best_model,f_list):\n",
    "    print('start')\n",
    "    best_model.dummy_ = \"dummy\"\n",
    "    validation.check_is_fitted(estimator=best_model)\n",
    "    my_plots =plot_partial_dependence(best_model, features=[f_list], X=raw_param, percentiles=(0, 1),grid_resolution=100)\n",
    "def pdp_plot_2d_ANN(model,f_list):\n",
    "    print('start')\n",
    "    model.dummy_ = \"dummy\"\n",
    "    print(type(model))\n",
    "    validation.check_is_fitted(estimator=model)\n",
    "    my_plots =plot_partial_dependence(model, features=[f_listt], X=raw_input, percentiles=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=9\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_param, raw_power, test_size=.1,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########LGBM gridsearch CV for best hyperparameter##########\n",
    "model_LightGBMRegressor=lightgbm.LGBMRegressor(random_state=1,verbose=-1)\n",
    "param_light = {\n",
    " 'boosting_type':['gbdt'],\n",
    " 'learning_rate':[0.1],\n",
    " 'n_estimators':[200],\n",
    "    'max_depth':[7],\n",
    " 'subsample':[0.4],\n",
    " 'reg_alpha':[0.001],\n",
    " 'reg_lambda':[0.001]\n",
    "}\n",
    "LGBM=gridsearch(model_LightGBMRegressor,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "##########XGBoost gridsearch CV for best hyperparameter##########\n",
    "model_XGBRegressor=xgboost.XGBRegressor(objective='reg:squarederror',random_state=1,verbose=0)\n",
    "param_xg = {\n",
    " 'booster':['gbtree'],\n",
    " 'learning_rate':[0.025],\n",
    " 'n_estimators':[400],\n",
    " 'max_depth':[9],\n",
    " 'subsample':[0.4],\n",
    " 'reg_alpha':[0.001],\n",
    " 'reg_lambda':[0.001]\n",
    "}\n",
    "XG=gridsearch(model_XGBRegressor,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########CatBoost gridsearch CV for best hyperparameter##########\n",
    "model_CatRegressor=catboost.CatBoostRegressor(random_state=1,verbose=0)\n",
    "param_cat = {\n",
    " 'learning_rate':[0.075],\n",
    " 'n_estimators':[400],\n",
    " 'max_depth':[5],\n",
    " 'subsample':[0.4],\n",
    "    'reg_lambda':[1e-05]\n",
    "}\n",
    "CAT=gridsearch(model_CatRegressor,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########import packages##########\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout \n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.constraints import maxnorm \n",
    "###########data standardization##########\n",
    "standardized_data = (raw_data-np.mean(raw_data,axis=0))/np.std(raw_data,axis=0)\n",
    "\n",
    "###########defining a wrapper function for later call from each machine learning algorithms##########\n",
    "raw_input=standardized_data.iloc[:,0:21]\n",
    "raw_output=standardized_data.iloc[:,21]\n",
    "X=raw_input.values.astype(np.float32)\n",
    "y=raw_output.values.astype(np.float32)\n",
    "raw_input_global=raw_data.iloc[:,0:21]\n",
    "raw_output_global=raw_data.iloc[:,21]\n",
    "###########wrap up fuction for later call for OPTIMIZATION##########\n",
    "def evaluate(pre_2,real_2):\n",
    "    pre_2=np.array(pre_2)\n",
    "    real_2=np.array(real_2)\n",
    "    pre_2_series=pd.Series(pre_2)\n",
    "    real_2_series=pd.Series(real_2)\n",
    "    return rmse(pre_2,real_2), round(pre_2_series.corr(real_2_series), 3)\n",
    "def compare(list_name,limit):\n",
    "    judge=1\n",
    "    for a in list_name:\n",
    "        if a < limit:\n",
    "            judge=judge*1\n",
    "        else:\n",
    "            judge=judge*0\n",
    "    return judge\n",
    "def generate_arrays_from_file(path):\n",
    "    while True:\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                # create numpy arrays of input data\n",
    "                # and labels, from each line in the file\n",
    "                x1, x2, y = process_line(line)\n",
    "                yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
    "def intergate(y_pred):\n",
    "    length=y_pred.shape[0]\n",
    "    print(length)\n",
    "    for i in range (0,length):\n",
    "        if y_pred[i][0]>=0.5:\n",
    "            y_pred[i][0]=1\n",
    "        else:\n",
    "            y_pred[i][0]=0\n",
    "    return y_pred\n",
    "early_stopping=keras.callbacks.EarlyStopping(\n",
    " monitor=\"val_loss\", \n",
    " patience=20, \n",
    " verbose=0, \n",
    " mode=\"auto\"\n",
    ")\n",
    "for batch_size_number in [24]:\n",
    "    for reg in [0.0001]:\n",
    "        for dropout_rate in [0]:\n",
    "            for neurons1 in [300]:\n",
    "                for epochs_number in [550]:\n",
    "                    for learning_rate_search in [0.02]:\n",
    "                        for activation1 in ['relu']:\n",
    "                            regularizer=keras.regularizers.l2(reg)\n",
    "                            ###########keras ANN model construction##########\n",
    "                            model = Sequential() \n",
    "                            model.add(Dense(neurons1, input_dim=21, kernel_initializer='random_normal',\n",
    "                                            bias_initializer='random_normal',activation=activation1,kernel_regularizer=regularizer)) \n",
    "                            model.add(Dropout(dropout_rate))\n",
    "                            model.add(Dense(neurons1, input_dim=neurons1, kernel_initializer='random_normal',\n",
    "                                            bias_initializer='random_normal',activation=activation1,kernel_regularizer=regularizer)) \n",
    "                            model.add(Dropout(dropout_rate))\n",
    "                            model.add(Dense(1, input_dim=neurons1, activation='linear'))\n",
    "                            adam=optimizers.Adam(lr=learning_rate_search)\n",
    "                            model.compile(loss='mse', optimizer=adam)\n",
    "                            print('training...')\n",
    "                            model.fit(X_train, y_train,verbose=0, batch_size=batch_size_number,epochs=epochs_number,validation_split=0.2,callbacks=[early_stopping])\n",
    "                            result=model.predict(X_test)\n",
    "                            result_train=model.predict(X_train)\n",
    "#                             ###########get RMSE and R2 on the test set##########\n",
    "#                             x_prediction_07=result*np.std(raw_output_global,axis=0)+np.mean(raw_output_global,axis=0)\n",
    "#                             y_real_07=np.std(raw_output_global,axis=0)*y_test+np.mean(raw_output_global,axis=0)\n",
    "#                             x_prediction_07_series=pd.Series(x_prediction_07[:,0])\n",
    "#                             y_real_07_series=pd.Series(y_real_07)\n",
    "#                             #training set\n",
    "#                             x_prediction_07_train=result_train*np.std(raw_output_global,axis=0)+np.mean(raw_output_global,axis=0)\n",
    "#                             y_real_07_train=np.std(raw_output_global,axis=0)*y_train+np.mean(raw_output_global,axis=0)\n",
    "#                             x_prediction_07_series_train=pd.Series(x_prediction_07_train[:,0])\n",
    "#                             y_real_07_series_train=pd.Series(y_real_07_train)\n",
    "#                             ###########evaluating the regression quality##########\n",
    "#                             corr_ann = round(x_prediction_07_series.corr(y_real_07_series), 5)\n",
    "#                             error_val= compute_mae_mse_rmse(x_prediction_07[:,0],y_real_07)\n",
    "#                             corr_ann_train = round(x_prediction_07_series_train.corr(y_real_07_series_train), 5)\n",
    "#                             error_val_train= compute_mae_mse_rmse(x_prediction_07_train[:,0],y_real_07_train)\n",
    "#                             print('TEST SET scatter corr',corr_ann,'scatter error',error_val,'TEST R2',error_val[3])\n",
    "#                             print('TRAINING SET scatter corr',corr_ann_train,'scatter error',error_val_train,'R2',error_val_train[3])\n",
    "#                             print(neurons1,epochs_number,learning_rate_search,dropout_rate,batch_size_number,reg,activation1)                      \n",
    "#                             x_y_x=np.arange(0.2,1.1,0.1)\n",
    "#                             x_y_y=np.arange(0.2,1.1,0.1)\n",
    "#                             fig = plt.figure()\n",
    "#                             ax = fig.add_subplot(111)\n",
    "#                             ax.scatter(x_prediction_07[:,0],y_real_07,color='red',label='Artificial Neural Network Test Set',alpha=0.75)\n",
    "#                             ax.scatter(x_prediction_07_train[:,0],y_real_07_train,color='blue',label='Artificial Neural Network Training Set',alpha=0.25,marker=\"^\")\n",
    "#                             ax.plot(x_y_x,x_y_y)\n",
    "#                             plt.legend()\n",
    "#                             plt.xlabel(u\"Predicted_Overpotential_@_Maximum_Faradic_Efficiency V (vs. RHE)\")\n",
    "#                             plt.ylabel(u\"Real_Overpotential_@_Maximum_Faradic_Efficiency V (vs. RHE)\")\n",
    "#                             plt.savefig('newest 0.678 %s %s %s %s %s %s %s.png' %(neurons1,epochs_number,learning_rate_search,dropout_rate,batch_size_number,reg,activation1))\n",
    "#                             plt.show()\n",
    "ANN=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_interaction(LGBM,algorithm_name=\"LightGBM\",interacted_features=['Flow Cell/H-type Cell','Catalyst Loading (mg cm-2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_plot_2d_XG_CAT(LGBM,['Flow Cell/H-type Cell','Catalyst Loading (mg cm-2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "shap_plot_interaction(XG,algorithm_name=\"XGBoost\",interacted_features=['Flow Cell/H-type Cell','Catalyst Loading (mg cm-2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_plot_2d_XG_CAT(XG,['Flow Cell/H-type Cell','Catalyst Loading (mg cm-2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_interaction(CAT,algorithm_name=\"CatBoost\",interacted_features=['Flow Cell/H-type Cell','Catalyst Loading (mg cm-2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_plot_2d_XG_CAT(CAT,['Flow Cell/H-type Cell','Catalyst Loading (mg cm-2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_interaction(ANN,algorithm_name=\"ANN\",interacted_features=['Flow Cell/H-type Cell','Catalyst Loading (mg cm-2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdp_interact_ANN(ANN,raw_input,['Flow Cell/H-type Cell','Catalyst Loading (mg cm-2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
