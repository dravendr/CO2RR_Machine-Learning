{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "ready\n"
     ]
    }
   ],
   "source": [
    "###########import packages##########\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import linear_model\n",
    "import lightgbm\n",
    "import catboost\n",
    "import xgboost\n",
    "import shap\n",
    "from catboost import *\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "%matplotlib\n",
    "###########loading data##########\n",
    "fdata=pd.read_csv('database_filled_TOF_MCL.csv',encoding=\"gbk\")\n",
    "raw_data=fdata.loc[:,[\n",
    "                      'Ionization Potential',#0\n",
    "                      'Electronegativity',#1\n",
    "                      'Number of d electrons',#2\n",
    "                      'Graphene/Carbon Nanosheets or other 2D Structures',#3\n",
    "                      'Carbon Nanofiber/Nanotubes',#4\n",
    "                      'Biomass or other Organic Derived',#5  \n",
    "                      'Main Transition Metal Content (wt. %)',#6\n",
    "                      'Nitrogen Cotent (wt. %)',#7\n",
    "                      'Metal-N Coordination Number (XAS)',#8    \n",
    "                      'Pyridinic N Ratio',#9\n",
    "                      'Pyrrolic N Ratio',#10\n",
    "                      'Raman ID/IG Ratio',#11\n",
    "                      'BET Surface Area (m2/g)',#12\n",
    "                      'Pyrolysis Temperature (°C)',#13\n",
    "                      'Pyrolysis Time (h)',#14\n",
    "                      'Rising Rate (°C min-1)',#15\n",
    "                      'Flow Cell/H-type Cell',#16\n",
    "                      'Electrolyte Concentration (M)',#17\n",
    "                      'Catalyst Loading (mg cm-2)',#18\n",
    "                      'Carbon Paper/Glassy Carbon',#19\n",
    "                      'Electrolyte pH',#20\n",
    "                      'TOF_MCL_2_4'#21\n",
    "                        ]]\n",
    "###########train test splitting##########\n",
    "raw_param=raw_data.iloc[:,0:21]\n",
    "raw_power=raw_data.iloc[:,21]\n",
    "print('ready')\n",
    "nb_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import *\n",
    "def gridsearch(model,param,algorithm_name):\n",
    "    print('start')\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5,n_jobs=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    print('Best Classifier:',grid.best_params_,'Best Score:', grid.best_score_)\n",
    "    best_model=grid.best_estimator_\n",
    "    prediction_train=best_model.predict(X_train)\n",
    "    prediction_test=best_model.predict(X_test)\n",
    "    final_result=classification_report(y_test,prediction_test,output_dict=True)\n",
    "    ##############################################################\n",
    "    Y_test_labeled = label_binarize(y_test, classes=[i for i in range(nb_classes)])\n",
    "    y_score=best_model.predict_proba(X_test)\n",
    "#     y_score=y_score[:,1]\n",
    "#     print(y_score)\n",
    "    auc_curve(Y_test_labeled,y_score,algorithm_name)\n",
    "    ##############################################################\n",
    "    print(classification_report(y_train,prediction_train))\n",
    "    print(classification_report(y_test,prediction_test))\n",
    "    print(final_result['accuracy'])\n",
    "    ###########generating a figure##########\n",
    "    print(algorithm_name)\n",
    "    print(best_model.feature_importances_)\n",
    "def auc_curve(y_label, y_pre,algorithm_name):\n",
    "#     y_label = y_label + 1\n",
    "#     y_pre = y_pre + 1\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(nb_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_label[:, i], y_pre[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_label.ravel(), y_pre.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(nb_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(nb_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= nb_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    lw = 2\n",
    "    fig=plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(nb_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (Multi-Class Classification) of %s' %algorithm_name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('ROC Curve of %s TOFMCL 3D.png' %algorithm_name)\n",
    "def shap_plot_multi(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:21]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,21]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        original_shape = shap_values.shape\n",
    "        shap_values_reshaped = shap_values.reshape(original_shape[1], original_shape[0], original_shape[-1])\n",
    "        shap.summary_plot(list(shap_values_reshaped[:,:,:-1]), SHAP_INPUT,max_display=100)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "    elif algorithm_name=='LightGBM':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)        \n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "def shap_plot_0(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:21]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,21]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        original_shape = shap_values.shape\n",
    "        shap_values_reshaped = shap_values.reshape(original_shape[1], original_shape[0], original_shape[-1])\n",
    "        shap.summary_plot(list(shap_values_reshaped[:,:,:-1])[0], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(list(shap_values_reshaped[:,:,:-1])[0]).mean(0)\n",
    "        print(global_importances)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[0], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[0]).mean(0)\n",
    "        print(global_importances)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[0], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[0]).mean(0)\n",
    "        print(global_importances)\n",
    "def shap_plot_1(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:21]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,21]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        original_shape = shap_values.shape\n",
    "        shap_values_reshaped = shap_values.reshape(original_shape[1], original_shape[0], original_shape[-1])\n",
    "        shap.summary_plot(list(shap_values_reshaped[:,:,:-1])[1], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(list(shap_values_reshaped[:,:,:-1])[1]).mean(0)\n",
    "        print(global_importances)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[1], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[1]).mean(0)\n",
    "        print(global_importances)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[1], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[1]).mean(0)\n",
    "        print(global_importances)\n",
    "def shap_plot_2(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:21]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,21]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        original_shape = shap_values.shape\n",
    "        shap_values_reshaped = shap_values.reshape(original_shape[1], original_shape[0], original_shape[-1])\n",
    "        shap.summary_plot(list(shap_values_reshaped[:,:,:-1])[2], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(list(shap_values_reshaped[:,:,:-1])[2]).mean(0)\n",
    "        print(global_importances)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[2], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[2]).mean(0)\n",
    "        print(global_importances)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[2], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[2]).mean(0)\n",
    "        print(global_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_param, raw_power, test_size=.1,random_state=seed)\n",
    "#########CatBoost gridsearch CV for best hyperparameter##########\n",
    "model_CatClassifier=catboost.CatBoostClassifier(random_state=1,verbose=0,bootstrap_type='Bernoulli')\n",
    "param_cat = {\n",
    " 'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    " 'n_estimators':[50,100,200,400],\n",
    " \"boosting_type\":[\"Plain\"],\n",
    " 'max_depth':[5,7,9,11],\n",
    " 'subsample':[0.4,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "    'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "gridsearch(model_CatClassifier,param_cat,'CatBoost')\n",
    "\n",
    "model_LightGBMClassifier=lightgbm.LGBMClassifier(random_state=1,verbose=-1)\n",
    "param_light = {\n",
    "  'boosting_type':['gbdt','rf'],\n",
    "    'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "    'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "     'n_estimators':[50,100,200,400],\n",
    "    'max_depth':[5,7,9,11,-1],\n",
    " 'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    " 'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "gridsearch(model_LightGBMClassifier,param_light,'LightGBM')\n",
    "\n",
    "##########XGBoost gridsearch CV for best hyperparameter##########\n",
    "model_XGBClassifier=xgboost.XGBClassifier(objective ='reg:squarederror',random_state=1,verbosity=0)\n",
    "param_xg = {\n",
    " 'booster':['gbtree'],\n",
    "  'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    " 'n_estimators':[50,100,200,400],\n",
    " 'max_depth':[5,7,9,11,16],\n",
    " 'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    " 'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    " 'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "gridsearch(model_XGBClassifier,param_xg,'XGBoost')\n",
    "###########GradientBoost gridsearch CV for best hyperparameter##########\n",
    "model_GradientBoostingClassifier = ensemble.GradientBoostingClassifier(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_GB = {\n",
    " 'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "  'n_estimators':[50,100,200,400],\n",
    " 'max_depth':[3,5,7,9,11,13,16],\n",
    " 'criterion':['friedman_mse','mae','mse'],\n",
    " 'max_features':['auto','sqrt','log2'],\n",
    " 'loss':['deviance', 'exponential']\n",
    "}\n",
    "gridsearch(model_GradientBoostingClassifier,param_GB,'GradientBoost')\n",
    "\n",
    "###########RandomForest gridsearch CV for best hyperparameter##########\n",
    "model_RandomForestClassifier = ensemble.RandomForestClassifier(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_RF = {\n",
    "      'n_estimators':[10,50,100,200,400],\n",
    "     'max_depth':[3,5,7,9,11,None],\n",
    "     'criterion':['gini','entropy'],\n",
    "     'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "gridsearch(model_RandomForestClassifier,param_RF,'Random Forest')\n",
    "\n",
    "\n",
    "###########Extra Tree gridsearch CV for best hyperparameter##########\n",
    "model_ExtraTreeClassifier = ExtraTreeClassifier(random_state=1)\n",
    "param_ET = {\n",
    "        'max_depth':[5,6,7,8,9,10,11,None],\n",
    "        'criterion' : ['gini','entropy'],\n",
    "        'splitter' : [ \"best\",'random'],\n",
    "        'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "gridsearch(model_ExtraTreeClassifier,param_ET,'Extra Tree')\n",
    "\n",
    "\n",
    "###########Decision Tree gridsearch CV for best hyperparameter##########\n",
    "model_DecisionTreeClassifier = tree.DecisionTreeClassifier(random_state=1)\n",
    "param_DT = {\n",
    "        'max_depth':[5,6,7,8,9,10,11,None],\n",
    "        'criterion' : ['gini','entropy'],\n",
    "        'splitter' : [ \"best\",'random'],\n",
    "        'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "gridsearch(model_DecisionTreeClassifier,param_DT,'Decision Tree')\n",
    "\n",
    "\n",
    "###########AdaBoost gridsearch CV for best hyperparameter##########\n",
    "model_AdaBoostClassifier = ensemble.AdaBoostClassifier(random_state=1)\n",
    "param_Ada = {\n",
    "      'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "    'n_estimators':[50,100,200,400]\n",
    "}\n",
    "gridsearch(model_AdaBoostClassifier,param_Ada,'AdaBoost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_TOFMCL=pd.concat([X_train,y_train],axis=1)\n",
    "test_TOFMCL=pd.concat([X_test,y_test],axis=1)\n",
    "train_TOFMCL.to_csv(\"train_TOFMCL.csv\", index_label=\"index_label\")\n",
    "test_TOFMCL.to_csv(\"test_TOFMCL.csv\", index_label=\"index_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Best Classifier: {} Best Score: 0.8153167602245389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        88\n",
      "           1       1.00      1.00      1.00       327\n",
      "           2       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           1.00       433\n",
      "   macro avg       1.00      1.00      1.00       433\n",
      "weighted avg       1.00      1.00      1.00       433\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.50      0.64        16\n",
      "           1       0.74      0.97      0.84        30\n",
      "           2       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.78        49\n",
      "   macro avg       0.88      0.60      0.66        49\n",
      "weighted avg       0.81      0.78      0.75        49\n",
      "\n",
      "0.7755102040816326\n",
      "LightGBM\n",
      "[ 93 182  68  33  13   0 986 733 276 545 472 456 665 282 114 210  75  44\n",
      " 376  22  46]\n"
     ]
    }
   ],
   "source": [
    "##########LightGBM gridsearch CV for best hyperparameter##########\n",
    "model_LightGBMClassifier=lightgbm.LGBMClassifier(random_state=1,verbose=-1)\n",
    "param_light = {\n",
    "'boosting_type':['gbdt'],\n",
    " 'learning_rate':[1],\n",
    "  'n_estimators':[50],\n",
    " 'subsample':[0.4],\n",
    " 'max_depth':[5],\n",
    " 'reg_alpha':[0.01],\n",
    " 'reg_lambda':[0]\n",
    "}\n",
    "gridsearch(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 1438/1446 [00:14<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01132402 0.8166606  0.11091536 0.02276346 0.0302935  0.\n",
      " 1.50077325 0.26742055 0.26316762 0.16189225 0.29792282 0.24878463\n",
      " 0.32114117 0.323368   0.1144842  0.09159662 0.         0.10816252\n",
      " 0.41459635 0.02099891 0.02348022]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_0(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|=================== | 1363/1446 [00:13<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03593038 0.4176627  0.07315516 0.01971146 0.06474951 0.01796068\n",
      " 0.48937833 0.31161941 0.17047535 0.34793359 0.18886903 0.17130963\n",
      " 0.42551555 0.37084609 0.10101471 0.14610546 0.         0.01525504\n",
      " 0.08723726 0.0086806  0.05833854]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_1(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|=================== | 1382/1446 [00:14<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.36575566e-01 1.51252329e-01 3.92362065e-02 1.15557310e-04\n",
      " 0.00000000e+00 0.00000000e+00 1.18631250e+00 3.32225637e-01\n",
      " 1.29885483e-01 2.18421414e-01 1.10485687e-01 7.87520548e-02\n",
      " 2.91078418e-01 2.55226385e-02 4.85757375e-02 5.14364107e-02\n",
      " 1.84828101e-01 2.94522906e-05 1.63155343e+00 1.30110052e-02\n",
      " 4.39435440e-02]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_2(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TreeEnsemble' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-fdd2c0860e11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshap_plot_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_LightGBMClassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_light\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'LightGBM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-81314dd98db3>\u001b[0m in \u001b[0;36mshap_plot_multi\u001b[1;34m(model, param, algorithm_name)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSHAP_INPUT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_display\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0malgorithm_name\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'LightGBM'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mexplainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_SHAP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0mshap_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_SHAP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSHAP_INPUT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_display\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\shap\\explainers\\_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, data, model_output, feature_perturbation, feature_names, **deprecated_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpected_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 raise Exception(\"Currently TreeExplainer can only handle models with categorical splits when \" \\\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\shap\\explainers\\_tree.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, y, output, tree_limit)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtree_limit\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtree_limit\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m             \u001b[0mtree_limit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"logloss\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TreeEnsemble' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "shap_plot_multi(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Best Classifier: {'booster': 'gbtree', 'learning_rate': 0.25, 'max_depth': 11, 'n_estimators': 50, 'reg_alpha': 0.01, 'reg_lambda': 0.01, 'subsample': 0.7} Best Score: 0.8035017375033414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "           1       1.00      1.00      1.00       313\n",
      "           2       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00       433\n",
      "   macro avg       1.00      1.00      1.00       433\n",
      "weighted avg       1.00      1.00      1.00       433\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      0.98      0.99        44\n",
      "           2       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.98        49\n",
      "   macro avg       0.83      0.99      0.89        49\n",
      "weighted avg       0.99      0.98      0.98        49\n",
      "\n",
      "0.9795918367346939\n",
      "XGBoost\n",
      "[0.046489   0.06863233 0.05284328 0.05289664 0.02747066 0.07511106\n",
      " 0.08312552 0.04905857 0.04357879 0.04199241 0.02976919 0.0314338\n",
      " 0.03616277 0.06940895 0.03613929 0.03705021 0.06095274 0.02871224\n",
      " 0.05386861 0.03295982 0.04234412]\n"
     ]
    }
   ],
   "source": [
    "##########XGBoost gridsearch CV for best hyperparameter##########\n",
    "model_XGBClassifier=xgboost.XGBClassifier(objective ='reg:squarederror',random_state=1,verbosity=0)\n",
    "param_xg = {\n",
    " 'booster':['gbtree'],\n",
    "  'learning_rate':[0.25],\n",
    " 'n_estimators':[50],\n",
    " 'max_depth':[11],\n",
    " 'subsample':[0.7],\n",
    " 'reg_alpha':[0.01],\n",
    " 'reg_lambda':[0.01]\n",
    "}\n",
    "gridsearch(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "[0.06535396 0.66046987 0.1333882  0.01715981 0.01395146 0.00571751\n",
      " 1.23869727 0.201422   0.12092518 0.12610314 0.23700581 0.24635778\n",
      " 0.32053303 0.2937177  0.04636378 0.07246652 0.         0.03071402\n",
      " 0.32397963 0.00714986 0.05730226]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_0(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "[0.08509234 0.28631622 0.07826487 0.02517312 0.01350372 0.02557901\n",
      " 0.46512055 0.27019587 0.09601048 0.20798453 0.15905295 0.14864613\n",
      " 0.29333524 0.2624804  0.08413663 0.13422298 0.0107461  0.02582098\n",
      " 0.15074727 0.0330624  0.05678398]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_1(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "[0.00894828 0.03050933 0.00515887 0.         0.         0.\n",
      " 0.8322222  0.15518842 0.12305332 0.11565768 0.10220103 0.02894086\n",
      " 0.16371496 0.06271433 0.01837712 0.06054756 0.05504554 0.\n",
      " 0.74607821 0.00356229 0.09742016]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_2(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n"
     ]
    }
   ],
   "source": [
    "shap_plot_multi(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Best Classifier: {'boosting_type': 'Plain', 'learning_rate': 0.25, 'max_depth': 7, 'n_estimators': 50, 'reg_lambda': 0.01, 'subsample': 0.9} Best Score: 0.8082865543972201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "           1       1.00      1.00      1.00       313\n",
      "           2       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00       433\n",
      "   macro avg       1.00      1.00      1.00       433\n",
      "weighted avg       1.00      1.00      1.00       433\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.50      0.57         4\n",
      "           1       0.96      0.98      0.97        44\n",
      "           2       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.94        49\n",
      "   macro avg       0.87      0.83      0.85        49\n",
      "weighted avg       0.93      0.94      0.93        49\n",
      "\n",
      "0.9387755102040817\n",
      "CatBoost\n",
      "[ 4.03922466  4.21494319  5.79975584  0.82189707  0.29947581  0.2871778\n",
      " 14.3992529   9.0005866   5.39811207  6.16638725  6.14772896  8.49177024\n",
      "  8.48069352  6.41322397  2.96936961  3.62709648  0.83245499  2.63077124\n",
      "  7.09240259  0.66943117  2.21824404]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    }
   ],
   "source": [
    "##########CatBoost gridsearch CV for best hyperparameter##########\n",
    "model_CatClassifier=catboost.CatBoostClassifier(random_state=1,verbose=0,bootstrap_type='Bernoulli')\n",
    "param_cat = {\n",
    " 'learning_rate':[0.25],\n",
    " 'n_estimators':[50],\n",
    " \"boosting_type\":[\"Plain\"],\n",
    " 'max_depth':[7],\n",
    " 'subsample':[0.9],\n",
    "    'reg_lambda':[0.01]\n",
    "}\n",
    "gridsearch(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n",
      "[0.13957615 0.19425297 0.205807   0.03239226 0.00960438 0.00792466\n",
      " 0.5788552  0.14827774 0.1411862  0.12285437 0.1137175  0.16641786\n",
      " 0.17023254 0.19303377 0.07687125 0.07096162 0.02478764 0.05700155\n",
      " 0.33388054 0.0227284  0.03931203]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_0(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n",
      "[0.13817912 0.19445972 0.20292496 0.01886157 0.01074085 0.00451635\n",
      " 0.56822999 0.15764429 0.14217378 0.12265568 0.1160882  0.18307446\n",
      " 0.17115033 0.17728127 0.08947215 0.08182225 0.05598559 0.06826475\n",
      " 0.29142488 0.02433868 0.06857625]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_1(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n",
      "[0.13664685 0.18697482 0.20009322 0.0262835  0.01190404 0.01235377\n",
      " 0.54829496 0.17072186 0.11529737 0.1326679  0.13860238 0.17982819\n",
      " 0.16501165 0.19363148 0.07930614 0.06848395 0.03826074 0.05840805\n",
      " 0.31579643 0.02406742 0.05317455]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_2(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n"
     ]
    }
   ],
   "source": [
    "shap_plot_multi(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########GradientBoost gridsearch CV for best hyperparameter##########\n",
    "model_GradientBoostingClassifier = ensemble.GradientBoostingClassifier(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_GB = {\n",
    " 'learning_rate':[1],\n",
    "  'n_estimators':[100],\n",
    " 'max_depth':[9],\n",
    " 'criterion':['mse'],\n",
    " 'max_features':['auto'],\n",
    " 'loss':['deviance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch(model_GradientBoostingClassifier,param_GB,'GradientBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Best Classifier: {'criterion': 'entropy', 'max_depth': 9, 'max_features': 'auto', 'n_estimators': 50} Best Score: 0.7898957497995189\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       100\n",
      "           1       0.96      1.00      0.98       313\n",
      "           2       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.97       433\n",
      "   macro avg       0.99      0.95      0.97       433\n",
      "weighted avg       0.97      0.97      0.97       433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       0.96      1.00      0.98        44\n",
      "           2       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.96        49\n",
      "   macro avg       0.99      0.83      0.88        49\n",
      "weighted avg       0.96      0.96      0.95        49\n",
      "\n",
      "0.9591836734693877\n",
      "Random Forest\n",
      "[0.03593535 0.03448052 0.03148879 0.00583281 0.00574446 0.0048983\n",
      " 0.21285444 0.10119102 0.05100105 0.06122213 0.05570763 0.0532571\n",
      " 0.07620486 0.05677921 0.02693375 0.03539242 0.01765068 0.01070004\n",
      " 0.08153075 0.00731733 0.03387735]\n"
     ]
    }
   ],
   "source": [
    "###########RandomForest gridsearch CV for best hyperparameter##########\n",
    "model_RandomForestClassifier = ensemble.RandomForestClassifier(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_RF = {\n",
    "      'n_estimators':[50],\n",
    "     'criterion':['entropy'],\n",
    "     'max_depth': [9],\n",
    "     'max_features':['auto']\n",
    "}\n",
    "gridsearch(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "[0.01470068 0.02871847 0.0197333  0.00146787 0.00204928 0.00112876\n",
      " 0.08575155 0.0274723  0.01366239 0.01541535 0.01363201 0.01037626\n",
      " 0.0213064  0.02773078 0.00564534 0.00704237 0.00189864 0.00345756\n",
      " 0.02501479 0.00302088 0.00818331]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_0(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "[0.01367715 0.02548913 0.01764782 0.00164329 0.0019794  0.00113492\n",
      " 0.06273928 0.03257124 0.01275189 0.0165626  0.0141511  0.01125806\n",
      " 0.02301667 0.0289365  0.00680624 0.00817824 0.00152027 0.00356975\n",
      " 0.02218361 0.00348606 0.00780392]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_1(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "[6.29737788e-03 5.61440728e-03 3.08753202e-03 4.06005782e-04\n",
      " 7.84002701e-04 8.08575387e-05 3.99955643e-02 6.89919961e-03\n",
      " 7.91397605e-03 3.70672466e-03 3.62263945e-03 5.04781651e-03\n",
      " 5.56997807e-03 4.60728614e-03 2.67042374e-03 3.35454314e-03\n",
      " 2.67226608e-03 1.02881729e-03 2.66077579e-02 9.29716672e-04\n",
      " 2.95442451e-03]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_2(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n"
     ]
    }
   ],
   "source": [
    "shap_plot_multi(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Best Classifier: {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'auto', 'splitter': 'random'} Best Score: 0.7689120556001069\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.30      0.43       100\n",
      "           1       0.77      0.96      0.86       313\n",
      "           2       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.77       433\n",
      "   macro avg       0.50      0.42      0.43       433\n",
      "weighted avg       0.73      0.77      0.72       433\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.25      0.22         4\n",
      "           1       0.91      0.91      0.91        44\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.84        49\n",
      "   macro avg       0.37      0.39      0.38        49\n",
      "weighted avg       0.83      0.84      0.83        49\n",
      "\n",
      "0.8367346938775511\n",
      "Extra Tree\n",
      "[0.03616244 0.         0.09978657 0.07045893 0.         0.\n",
      " 0.36518019 0.14235783 0.19200244 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.02359267\n",
      " 0.         0.         0.07045893]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "###########Extra Tree gridsearch CV for best hyperparameter##########\n",
    "model_ExtraTreeClassifier = ExtraTreeClassifier(random_state=1)\n",
    "param_ET = {\n",
    "        'max_depth':[5],\n",
    "        'criterion' : ['entropy'],\n",
    "        'splitter' : [ \"random\"],\n",
    "        'max_features':['auto']\n",
    "}\n",
    "gridsearch(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree\n",
      "[0.00694963 0.         0.02011061 0.00176003 0.         0.\n",
      " 0.07296932 0.0284549  0.02649758 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00552444\n",
      " 0.         0.         0.00193465]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_0(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree\n",
      "[0.00694963 0.         0.01480622 0.00176003 0.         0.\n",
      " 0.0524834  0.02671473 0.02401669 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00552444\n",
      " 0.         0.         0.00193465]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_1(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree\n",
      "[0.         0.         0.00530438 0.         0.         0.\n",
      " 0.02057958 0.00174016 0.00249817 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_2(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree\n"
     ]
    }
   ],
   "source": [
    "shap_plot_multi(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Best Classifier: {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'auto', 'splitter': 'random'} Best Score: 0.7689120556001069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.30      0.43       100\n",
      "           1       0.77      0.96      0.86       313\n",
      "           2       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.77       433\n",
      "   macro avg       0.50      0.42      0.43       433\n",
      "weighted avg       0.73      0.77      0.72       433\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.25      0.22         4\n",
      "           1       0.91      0.91      0.91        44\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.84        49\n",
      "   macro avg       0.37      0.39      0.38        49\n",
      "weighted avg       0.83      0.84      0.83        49\n",
      "\n",
      "0.8367346938775511\n",
      "Decision Tree\n",
      "[0.03616244 0.         0.09978657 0.07045893 0.         0.\n",
      " 0.36518019 0.14235783 0.19200244 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.02359267\n",
      " 0.         0.         0.07045893]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "###########Decision Tree gridsearch CV for best hyperparameter##########\n",
    "model_DecisionTreeClassifier = tree.DecisionTreeClassifier(random_state=1)\n",
    "param_DT = {\n",
    "        'max_depth':[5],\n",
    "        'criterion' : ['entropy'],\n",
    "        'splitter' : [ \"random\"],\n",
    "        'max_features':['auto']\n",
    "}\n",
    "gridsearch(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "[0.00694963 0.         0.02011061 0.00176003 0.         0.\n",
      " 0.07296932 0.0284549  0.02649758 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00552444\n",
      " 0.         0.         0.00193465]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_0(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "[0.00694963 0.         0.01480622 0.00176003 0.         0.\n",
      " 0.0524834  0.02671473 0.02401669 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00552444\n",
      " 0.         0.         0.00193465]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_1(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "[0.         0.         0.00530438 0.         0.         0.\n",
      " 0.02057958 0.00174016 0.00249817 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_2(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n"
     ]
    }
   ],
   "source": [
    "shap_plot_multi(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Best Classifier: {'learning_rate': 0.001, 'n_estimators': 50} Best Score: 0.759796845763165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.31      0.44       100\n",
      "           1       0.77      0.97      0.86       313\n",
      "           2       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.77       433\n",
      "   macro avg       0.51      0.43      0.43       433\n",
      "weighted avg       0.73      0.77      0.72       433\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       0.94      1.00      0.97        44\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.94        49\n",
      "   macro avg       0.65      0.50      0.54        49\n",
      "weighted avg       0.92      0.94      0.92        49\n",
      "\n",
      "0.9387755102040817\n",
      "AdaBoost\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "###########AdaBoost gridsearch CV for best hyperparameter##########\n",
    "model_AdaBoostClassifier = ensemble.AdaBoostClassifier(random_state=1)\n",
    "param_Ada = {\n",
    "      'learning_rate':[0.001],\n",
    "    'n_estimators':[50]\n",
    "}\n",
    "gridsearch(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.07431451 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_0(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.06728274 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_1(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.00703177 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "shap_plot_2(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n"
     ]
    }
   ],
   "source": [
    "shap_plot_multi(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
