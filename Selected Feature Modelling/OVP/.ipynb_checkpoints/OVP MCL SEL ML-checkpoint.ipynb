{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########import packages##########\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from itertools import cycle\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn import linear_model\n",
    "import lightgbm\n",
    "from scipy import interp\n",
    "import catboost\n",
    "import xgboost\n",
    "import shap\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from catboost import *\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "%matplotlib\n",
    "###########loading data##########\n",
    "fdata=pd.read_csv('database_filled.csv',encoding=\"gbk\")\n",
    "raw_data=fdata.loc[:,[\n",
    "                      'Ionization Potential',#0\n",
    "                      'Electronegativity',#1\n",
    "                      'Number of d electrons',#2\n",
    "                      'ZIF or MOF Derived',#3\n",
    "                      'Polymer Derived',#4\n",
    "                      'Carbon Nanofiber/Nanotubes',#5\n",
    "                      'Biomass or other Organic Derived',#6  \n",
    "                      'Main Transition Metal Content (wt. %)',#7\n",
    "                      'Nitrogen Cotent (wt. %)',#8\n",
    "                      'Metal-N Coordination Number (XAS)',#9    \n",
    "                      'Pyridinic N Ratio',#10\n",
    "                      'Pyrrolic N Ratio',#11\n",
    "                      'Raman ID/IG Ratio',#12\n",
    "                      'BET Surface Area (m2/g)',#13\n",
    "                      'Pyrolysis Temperature (°C)',#14\n",
    "                      'Pyrolysis Time (h)',#15\n",
    "                      'Rising Rate (°C min-1)',#16\n",
    "                      'Electrolyte Concentration (M)',#17\n",
    "                      'Catalyst Loading (mg cm-2)',#18\n",
    "                      'Nafion Membrane Thickness (μm)',#19\n",
    "                      'Carbon Paper/Glassy Carbon',#20\n",
    "                      'Electrolyte pH',#21\n",
    "                      'ovpCL',#22\n",
    "                        ]]\n",
    "###########train test splitting##########\n",
    "raw_param=raw_data.iloc[:,0:22]\n",
    "raw_power=raw_data.iloc[:,22]\n",
    "print('ready')\n",
    "nb_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from catboost import *\n",
    "def gridsearch(model,param,algorithm_name):\n",
    "    print('start')\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5,n_jobs=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    print('Best Classifier:',grid.best_params_,'Best Score:', grid.best_score_)\n",
    "    best_model=grid.best_estimator_\n",
    "    prediction_train=best_model.predict(X_train)\n",
    "    prediction_test=best_model.predict(X_test)\n",
    "    final_result=classification_report(y_test,prediction_test,output_dict=True)\n",
    "    ##############################################################\n",
    "    Y_test_labeled = label_binarize(y_test, classes=[i for i in range(nb_classes)])\n",
    "    y_score=best_model.predict_proba(X_test)\n",
    "#     y_score=y_score[:,1]\n",
    "#     print(y_score)\n",
    "    auc_curve(Y_test_labeled,y_score,algorithm_name)\n",
    "    ##############################################################\n",
    "    print(classification_report(y_train,prediction_train))\n",
    "    print(classification_report(y_test,prediction_test))\n",
    "    print(final_result['accuracy'])\n",
    "    ###########generating a figure##########\n",
    "    print(algorithm_name)\n",
    "    print(best_model.feature_importances_)\n",
    "def auc_curve(y_label, y_pre,algorithm_name):\n",
    "#     y_label = y_label + 1\n",
    "#     y_pre = y_pre + 1\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(nb_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_label[:, i], y_pre[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_label.ravel(), y_pre.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(nb_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(nb_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= nb_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    lw = 2\n",
    "    fig=plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(nb_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (Multi-Class Classification) of %s' %algorithm_name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('ROC Curve of %s OVPMCL.png' %algorithm_name)\n",
    "def shap_plot_multi(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:22]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,22]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "    \n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        original_shape = shap_values.shape\n",
    "        shap_values_reshaped = shap_values.reshape(original_shape[1], original_shape[0], original_shape[-1])\n",
    "        shap.summary_plot(list(shap_values_reshaped[:,:,:-1]), SHAP_INPUT,max_display=100)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "def shap_plot_0(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:22]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,22]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "   \n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        original_shape = shap_values.shape\n",
    "        shap_values_reshaped = shap_values.reshape(original_shape[1], original_shape[0], original_shape[-1])\n",
    "        shap.summary_plot(list(shap_values_reshaped[:,:,:-1])[0], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(list(shap_values_reshaped[:,:,:-1])[0]).mean(0)\n",
    "        print(global_importances)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[0], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[0]).mean(0)\n",
    "        print(global_importances)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[0], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[0]).mean(0)\n",
    "        print(global_importances)\n",
    "def shap_plot_1(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:22]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,22]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        original_shape = shap_values.shape\n",
    "        shap_values_reshaped = shap_values.reshape(original_shape[1], original_shape[0], original_shape[-1])\n",
    "        shap.summary_plot(list(shap_values_reshaped[:,:,:-1])[1], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(list(shap_values_reshaped[:,:,:-1])[1]).mean(0)\n",
    "        print(global_importances)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[1], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[1]).mean(0)\n",
    "        print(global_importances)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[1], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[1]).mean(0)\n",
    "        print(global_importances)\n",
    "def shap_plot_2(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:22]\n",
    "    SHAP_OUTPUT=raw_data.iloc[:,22]\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "   \n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        original_shape = shap_values.shape\n",
    "        shap_values_reshaped = shap_values.reshape(original_shape[1], original_shape[0], original_shape[-1])\n",
    "        shap.summary_plot(list(shap_values_reshaped[:,:,:-1])[2], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(list(shap_values_reshaped[:,:,:-1])[2]).mean(0)\n",
    "        print(global_importances)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[2], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[2]).mean(0)\n",
    "        print(global_importances)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values[2], SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values[2]).mean(0)\n",
    "        print(global_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=184\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_param, raw_power, test_size=.1,random_state=seed)\n",
    "\n",
    "\n",
    "##########CatBoost gridsearch CV for best hyperparameter##########\n",
    "model_CatClassifier=catboost.CatBoostClassifier(random_state=1,verbose=0,bootstrap_type='Bernoulli')\n",
    "param_cat = {\n",
    " 'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    " 'n_estimators':[50,100,200,400],\n",
    " \"boosting_type\":[\"Plain\"],\n",
    " 'max_depth':[5,7,9,11],\n",
    " 'subsample':[0.4,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "    'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "gridsearch(model_CatClassifier,param_cat,'CatBoost')\n",
    "##########LGBM gridsearch CV for best hyperparameter##########\n",
    "model_LightGBMClassifier=lightgbm.LGBMClassifier(random_state=1,verbose=-1)\n",
    "param_light = {\n",
    "  'boosting_type':['gbdt','rf'],\n",
    "    'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "    'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "     'n_estimators':[50,100,200,400],\n",
    "    'max_depth':[5,7,9,11,-1],\n",
    " 'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    " 'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "gridsearch(model_LightGBMClassifier,param_light,'LightGBM')\n",
    "\n",
    "##########XGBoost gridsearch CV for best hyperparameter##########\n",
    "model_XGBClassifier=xgboost.XGBClassifier(objective ='reg:squarederror',random_state=1,verbosity=0)\n",
    "param_xg = {\n",
    " 'booster':['gbtree'],\n",
    "  'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    " 'n_estimators':[50,100,200,400],\n",
    " 'max_depth':[5,7,9,11,16],\n",
    " 'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    " 'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    " 'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "gridsearch(model_XGBClassifier,param_xg,'XGBoost')\n",
    "###########GradientBoost gridsearch CV for best hyperparameter##########\n",
    "model_GradientBoostingClassifier = ensemble.GradientBoostingClassifier(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_GB = {\n",
    " 'learning_rate':[0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1],\n",
    "  'n_estimators':[50,100,200,400],\n",
    " 'max_depth':[3,5,7,9,11,13,16],\n",
    " 'criterion':['friedman_mse','mae','mse'],\n",
    " 'max_features':['auto','sqrt','log2'],\n",
    " 'loss':['deviance', 'exponential']\n",
    "}\n",
    "gridsearch(model_GradientBoostingClassifier,param_GB,'GradientBoost')\n",
    "\n",
    "###########RandomForest gridsearch CV for best hyperparameter##########\n",
    "model_RandomForestClassifier = ensemble.RandomForestClassifier(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_RF = {\n",
    "      'n_estimators':[10,50,100,200,400],\n",
    "     'max_depth':[3,5,7,9,11,None],\n",
    "     'criterion':['gini','entropy'],\n",
    "     'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "gridsearch(model_RandomForestClassifier,param_RF,'Random Forest')\n",
    "\n",
    "\n",
    "###########Extra Tree gridsearch CV for best hyperparameter##########\n",
    "model_ExtraTreeClassifier = ExtraTreeClassifier(random_state=1)\n",
    "param_ET = {\n",
    "        'max_depth':[5,6,7,8,9,10,11,None],\n",
    "        'criterion' : ['gini','entropy'],\n",
    "        'splitter' : [ \"best\",'random'],\n",
    "        'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "gridsearch(model_ExtraTreeClassifier,param_ET,'Extra Tree')\n",
    "\n",
    "\n",
    "###########Decision Tree gridsearch CV for best hyperparameter##########\n",
    "model_DecisionTreeClassifier = tree.DecisionTreeClassifier(random_state=1)\n",
    "param_DT = {\n",
    "        'max_depth':[5,6,7,8,9,10,11,None],\n",
    "        'criterion' : ['gini','entropy'],\n",
    "        'splitter' : [ \"best\",'random'],\n",
    "        'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "gridsearch(model_DecisionTreeClassifier,param_DT,'Decision Tree')\n",
    "\n",
    "\n",
    "###########AdaBoost gridsearch CV for best hyperparameter##########\n",
    "model_AdaBoostClassifier = ensemble.AdaBoostClassifier(random_state=1)\n",
    "param_Ada = {\n",
    "      'learning_rate':[0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5],\n",
    "    'n_estimators':[50,100,200,400]\n",
    "}\n",
    "gridsearch(model_AdaBoostClassifier,param_Ada,'AdaBoost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_OVPMCL=pd.concat([X_train,y_train],axis=1)\n",
    "test_OVPMCL=pd.concat([X_test,y_test],axis=1)\n",
    "train_OVPMCL.to_csv(\"train_OVPMCL.csv\", index_label=\"index_label\")\n",
    "test_OVPMCL.to_csv(\"test_OVPMCL.csv\", index_label=\"index_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########LightGBM gridsearch CV for best hyperparameter##########\n",
    "model_LightGBMClassifier=lightgbm.LGBMClassifier(random_state=1,verbose=-1)\n",
    "param_light = {\n",
    "'boosting_type':['gbdt'],\n",
    " 'learning_rate':[0.75],\n",
    "  'n_estimators':[50],\n",
    " 'subsample':[0.4],\n",
    " 'max_depth':[11],\n",
    " 'reg_alpha':[0],\n",
    " 'reg_lambda':[0.01]\n",
    "}\n",
    "gridsearch(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_multi(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_0(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_1(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_2(model_LightGBMClassifier,param_light,'LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########XGBoost gridsearch CV for best hyperparameter##########\n",
    "model_XGBClassifier=xgboost.XGBClassifier(objective ='reg:squarederror',random_state=1,verbosity=0)\n",
    "param_xg = {\n",
    " 'booster':['gbtree'],\n",
    "  'learning_rate':[0.25],\n",
    " 'n_estimators':[100],\n",
    " 'max_depth':[5],\n",
    " 'subsample':[0.5],\n",
    " 'reg_alpha':[0.01],\n",
    " 'reg_lambda':[0.0001]\n",
    "}\n",
    "gridsearch(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_multi(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_0(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_1(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_2(model_XGBClassifier,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########CatBoost gridsearch CV for best hyperparameter##########\n",
    "model_CatClassifier=catboost.CatBoostClassifier(random_state=1,verbose=0,bootstrap_type='Bernoulli')\n",
    "param_cat = {\n",
    " 'learning_rate':[0.075],\n",
    " 'n_estimators':[50],\n",
    " \"boosting_type\":[\"Plain\"],\n",
    " 'max_depth':[7],\n",
    " 'subsample':[0.75],\n",
    "    'reg_lambda':[0.01]\n",
    "}\n",
    "gridsearch(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_multi(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_0(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_1(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_2(model_CatClassifier,param_cat,'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########GradientBoost gridsearch CV for best hyperparameter##########\n",
    "model_GradientBoostingClassifier = ensemble.GradientBoostingClassifier(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_GB = {\n",
    " 'learning_rate':[0.05],\n",
    "  'n_estimators':[400],\n",
    " 'max_depth':[3],\n",
    " 'criterion':['friedman_mse'],\n",
    " 'max_features':['sqrt'],\n",
    " 'loss':['deviance']\n",
    "}\n",
    "# shap_plot_multi(model_GradientBoostingClassifier,param_GB,'GradientBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch(model_GradientBoostingClassifier,param_GB,'GradientBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########RandomForest gridsearch CV for best hyperparameter##########\n",
    "model_RandomForestClassifier = ensemble.RandomForestClassifier(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_RF = {\n",
    "      'n_estimators':[10],\n",
    "     'max_depth':[9],\n",
    "     'criterion':['entropy'],\n",
    "     'max_features':['auto']\n",
    "}\n",
    "gridsearch(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_multi(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_0(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_1(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_2(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch(model_RandomForestClassifier,param_RF,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Extra Tree gridsearch CV for best hyperparameter##########\n",
    "model_ExtraTreeClassifier = ExtraTreeClassifier(random_state=1)\n",
    "param_ET = {\n",
    "        'max_depth':[8],\n",
    "        'criterion' : ['entropy'],\n",
    "        'splitter' : [ \"best\"],\n",
    "        'max_features':['auto']\n",
    "}\n",
    "gridsearch(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_multi(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_0(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_1(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_2(model_ExtraTreeClassifier,param_ET,'Extra Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Decision Tree gridsearch CV for best hyperparameter##########\n",
    "model_DecisionTreeClassifier = tree.DecisionTreeClassifier(random_state=1)\n",
    "param_DT = {\n",
    "        'max_depth':[8],\n",
    "        'criterion' : ['entropy'],\n",
    "        'splitter' : [ \"best\"],\n",
    "        'max_features':['auto']\n",
    "}\n",
    "gridsearch(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_multi(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_0(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_1(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_2(model_DecisionTreeClassifier,param_DT,'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########AdaBoost gridsearch CV for best hyperparameter##########\n",
    "model_AdaBoostClassifier = ensemble.AdaBoostClassifier(random_state=1)\n",
    "param_Ada = {\n",
    "      'learning_rate':[0.005],\n",
    "    'n_estimators':[200]\n",
    "}\n",
    "gridsearch(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_multi(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_0(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_1(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_plot_2(model_AdaBoostClassifier,param_Ada,'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
