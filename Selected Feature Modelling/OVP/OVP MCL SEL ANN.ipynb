{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########import packages##########\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout \n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.constraints import maxnorm \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm\n",
    "import catboost\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import ensemble\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "import keras_metrics as km\n",
    "from keras.callbacks import EarlyStopping \n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "early_stopping=keras.callbacks.EarlyStopping(\n",
    " monitor=\"val_loss\", \n",
    " patience=20, \n",
    " verbose=0, \n",
    " mode=\"auto\"\n",
    ")\n",
    "%matplotlib\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "nb_classes=3\n",
    "###########loading data##########\n",
    "fdata=pd.read_csv('database_filled.csv',encoding=\"gbk\")\n",
    "raw_data=fdata.loc[:,[                      \n",
    "                       'Ionization Potential',#0\n",
    "                      'Electronegativity',#1\n",
    "                      'Number of d electrons',#2\n",
    "                      'ZIF or MOF Derived',#3\n",
    "                      'Polymer Derived',#4\n",
    "                      'Carbon Nanofiber/Nanotubes',#5\n",
    "                      'Biomass or other Organic Derived',#6  \n",
    "                      'Main Transition Metal Content (wt. %)',#7\n",
    "                      'Nitrogen Cotent (wt. %)',#8\n",
    "                      'Metal-N Coordination Number (XAS)',#9    \n",
    "                      'Pyridinic N Ratio',#10\n",
    "                      'Pyrrolic N Ratio',#11\n",
    "                      'Raman ID/IG Ratio',#12\n",
    "                      'BET Surface Area (m2/g)',#13\n",
    "                      'Pyrolysis Temperature (°C)',#14\n",
    "                      'Pyrolysis Time (h)',#15\n",
    "                      'Rising Rate (°C min-1)',#16\n",
    "                      'Electrolyte Concentration (M)',#17\n",
    "                      'Catalyst Loading (mg cm-2)',#18\n",
    "                      'Nafion Membrane Thickness (μm)',#19\n",
    "                      'Carbon Paper/Glassy Carbon',#20\n",
    "                      'Electrolyte pH',#21\n",
    "                      'ovpCL',#22\n",
    "                        ]]\n",
    "\n",
    "###########data standardization##########\n",
    "standardized_data = (raw_data-np.mean(raw_data,axis=0))/np.std(raw_data,axis=0)\n",
    "\n",
    "###########defining a wrapper function for later call from each machine learning algorithms##########\n",
    "raw_input=standardized_data.iloc[:,0:22]\n",
    "raw_output=raw_data.iloc[:,22]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(raw_output)\n",
    "# convert integers to dummy variables (one hot encoding)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "\n",
    "###########fix random seed for reproducability##########\n",
    "seed=184\n",
    "###########train test splitting##########\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_input, dummy_y, test_size=.1,random_state=seed)\n",
    "X_train_o, X_test_o, y_train_o, y_test_o = train_test_split(raw_input, raw_output, test_size=.1,random_state=seed)\n",
    "raw_input_global=raw_data.iloc[:,0:22]\n",
    "raw_output_global=raw_data.iloc[:,22]\n",
    "###########wrap up fuction for later call for OPTIMIZATION##########\n",
    "def auc_ANN(y_label,y_pre,neurons1,epochs_number,dropout_rate,batch_size_number,reg,act):  \n",
    "    #     y_label = y_label + 1\n",
    "#     y_pre = y_pre + 1\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(nb_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_label[:, i], y_pre[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_label.ravel(), y_pre.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(nb_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(nb_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= nb_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    lw = 2\n",
    "    fig=plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(nb_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (Multi-Class Classification) of Artificial Neural Network')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('ROC Curve of %s %s %s %s %s %s OVPMCL ANN.png' %(neurons1,epochs_number,dropout_rate,batch_size_number,reg,act))\n",
    "\n",
    "accuracy={}\n",
    "for neurons1 in [100,200,400,600,800]:\n",
    "    for dropout_rate in [0,0.25,0.5]:\n",
    "        for batch_size_number in [8,16,32]:\n",
    "            for reg in [0,0.0001,0.001]:\n",
    "                for act in ['sigmoid','tanh','relu','softsign']:                        \n",
    "                    for epochs_number in range(100,800,100):\n",
    "                        regularizer=keras.regularizers.l2(reg)\n",
    "                        ###########keras ANN model construction########## \n",
    "                        model = Sequential() \n",
    "                        model.add(Dense(neurons1, input_dim=22, kernel_initializer='random_normal',\n",
    "                                        bias_initializer='random_normal',activation=act,kernel_regularizer=regularizer)) \n",
    "                        model.add(Dropout(dropout_rate))                        \n",
    "                        model.add(Dense(neurons1, input_dim=neurons1, kernel_initializer='random_normal',\n",
    "                                        bias_initializer='random_normal',activation=act,kernel_regularizer=regularizer)) \n",
    "                        model.add(Dropout(dropout_rate))\n",
    "                        model.add(Dense(3, input_dim=neurons1, activation='softmax'))\n",
    "#                         model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "                        model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "                        model.fit(X_train, y_train,verbose=0, batch_size=batch_size_number,epochs=epochs_number,validation_split=0.2)\n",
    "                        print(neurons1,epochs_number,dropout_rate,batch_size_number,reg,act)\n",
    "                        test_pred = model.predict(X_test)\n",
    "                        train_pred = model.predict(X_train)\n",
    "                        id_test = np.argmax(test_pred, axis=1)\n",
    "                        id_train = np.argmax(train_pred, axis=1)\n",
    "                        y_score=model.predict_proba(X_test)\n",
    "                        print(classification_report(id_train,np.argmax(y_train, axis=1)))\n",
    "                        print(classification_report(id_test,np.argmax(y_test, axis=1)))\n",
    "                        final_result=classification_report(id_test,np.argmax(y_test, axis=1),output_dict=True)\n",
    "                        ac=final_result['accuracy']\n",
    "                        accuracy[ac]=[neurons1,epochs_number,dropout_rate,batch_size_number,reg,act]\n",
    "                        auc_ANN(y_test,y_score,neurons1,epochs_number,dropout_rate,batch_size_number,reg,act)\n",
    "                        K.clear_session()    \n",
    "# # # 800 300 0.5 8 0.001 softsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "def base_model():\n",
    "    tmodel = Sequential() \n",
    "    tmodel.add(Dense(600, input_dim=22, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='softsign',kernel_regularizer=keras.regularizers.l2(0.001))) \n",
    "    tmodel.add(Dropout(0))\n",
    "    tmodel.add(Dense(600, input_dim=600, kernel_initializer='random_normal',\n",
    "                bias_initializer='random_normal',activation='softsign',kernel_regularizer=keras.regularizers.l2(0.001))) \n",
    "    tmodel.add(Dropout(0))\n",
    "    tmodel.add(Dense(3, input_dim=600, activation='softmax'))\n",
    "    tmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return tmodel\n",
    "from eli5.sklearn import PermutationImportance\n",
    "my_model = KerasClassifier(build_fn=base_model,nb_epoch=500, batch_size=32, verbose= False)    \n",
    "my_model.fit(X_train, y_train,validation_split=0.2)\n",
    "perm = PermutationImportance(my_model, random_state=1,n_iter=10).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm,feature_names=X_train.columns.tolist(),top=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm,top=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########keras ANN model construction##########\n",
    "smodel = Sequential() \n",
    "smodel.add(Dense(600, input_dim=22, kernel_initializer='random_normal',\n",
    "                bias_initializer='random_normal',activation='softsign',kernel_regularizer=keras.regularizers.l2(0.001))) \n",
    "smodel.add(Dropout(0))\n",
    "smodel.add(Dense(600, input_dim=600, kernel_initializer='random_normal',\n",
    "            bias_initializer='random_normal',activation='softsign',kernel_regularizer=keras.regularizers.l2(0.001))) \n",
    "smodel.add(Dropout(0))\n",
    "smodel.add(Dense(3, input_dim=600, activation='softmax'))\n",
    "smodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "print('training...')\n",
    "smodel.fit(X_train, y_train,verbose=0, batch_size=32,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# %matplotlib\n",
    "SHAP_INPUT=standardized_data.iloc[:,0:22]\n",
    "SHAP_OUTPUT=raw_data.iloc[:,22]\n",
    "X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "explainer = shap.GradientExplainer(smodel,X_SHAP)\n",
    "\n",
    "shap_values = explainer.shap_values(X_SHAP)\n",
    "print(type(shap_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[2], SHAP_INPUT,max_display=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], SHAP_INPUT,max_display=100)\n",
    "print(np.abs(shap_values[0]).mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], SHAP_INPUT,max_display=100)\n",
    "print(np.abs(shap_values[1]).mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[2], SHAP_INPUT,max_display=100)\n",
    "print(np.abs(shap_values[2]).mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
